{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33622,"status":"ok","timestamp":1670983655399,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"},"user_tz":300},"id":"hSh-vsGQwAza","outputId":"84e2af6e-1925-47c7-cf9c-2125031e1271"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\",force_remount=True)\n","\n","from zipfile import ZipFile\n","! cp '/content/drive/MyDrive/ML_Capstone/drive-download.zip' '/content'\n","from zipfile import ZipFile\n","zip = ZipFile('/content/drive-download.zip')\n","zip.extractall()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njbLHwZV0vHq"},"outputs":[],"source":["# importing required packages\n","import os\n","import torch \n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets\n","import torchvision.transforms.functional as F\n","import torchvision.transforms as transforms\n","from torchvision.transforms import ToTensor\n","import numpy as np\n","import cv2\n","import pickle as pkl\n","import matplotlib.pyplot as plt\n","from torchvision import models\n","from PIL import Image\n","from torch.optim.lr_scheduler import StepLR\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import torchvision"]},{"cell_type":"markdown","metadata":{"id":"oD3nH-6i1vsg"},"source":["# Dataset and Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":264,"status":"ok","timestamp":1670983661416,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"},"user_tz":300},"id":"DEYD2JLU5izs","outputId":"1d5dc23a-672b-4e0b-d990-74ff30043055"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["# Set device to cuda if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"markdown","metadata":{"id":"180ylhvU_sTl"},"source":["# Lazy Loading"]},{"cell_type":"markdown","metadata":{"id":"HdBnI6RKBBLD"},"source":["data\n","-train\n","  - X\n","    - 1\n","      - rgb\n","        - 0.png\n","        - 1.png\n","        - 2.png\n","      - depth.npy\n","      - field_id.pkl\n","    \n","    - 2\n","\n","    - ...\n","  - Y\n","    - 1.npy\n","    - 2.npy\n","\n","-test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Kjw2im8zrug"},"outputs":[],"source":["class LazyLoadDataset(Dataset):\n","  def __init__(self,path,train=True,transform=None):\n","    self.train = train\n","    self.transform = transform\n","    path = path + (\"train/\" if self.train else \"test/\")\n","\n","    self.pathX = path + \"X/\"\n","    self.pathY = path + \"Y/\"\n","\n","    self.data = os.listdir(self.pathX)\n","\n","  def __getitem__(self,idx):\n","    f = self.data[idx]\n","\n","    # X\n","    # read rgb images\n","    #img0 = cv2.imread(self.pathX + f + \"/rgb/0.png\")/255\n","\n","    # read depth images\n","    depth = np.load(self.pathX + f + \"/depth.npy\")/1000\n","    depth = depth.transpose((1,2,0))\n","        \n","    if self.transform is not None:\n","      depth = self.transform(depth)\n","\n","    # read field ID\n","    field_id = pkl.load(open(self.pathX + f + \"/field_id.pkl\",\"rb\"))\n","\n","    if self.train==True:\n","      # Y \n","      Y = np.load(self.pathY + f + \".npy\")*1000\n","      return (depth.float().type(torch.float32),field_id),torch.from_numpy(Y).float().type(torch.float32)\n","\n","    else: # if test, there is no Y\n","      return depth.float().type(torch.float32),field_id\n","\n","  def __len__(self):\n","    return len(self.data)\n"]},{"cell_type":"markdown","metadata":{"id":"stlacrCtoQjh"},"source":["# Normalization: Compute Mean and Std\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srTzFVXxi1dU"},"outputs":[],"source":["train_dataset_raw = LazyLoadDataset(\"/content/\",train=True,transform=ToTensor()) \n","means=[]\n","stds=[]\n","for i in range(3396):\n","  (depth_i, id_i),Y_i=train_dataset_raw[i]\n","  means.append([torch.mean(depth_i[0]).item(),torch.mean(depth_i[1]).item(),torch.mean(depth_i[2]).item()])\n","  stds.append([torch.std(depth_i[0]).item(),torch.std(depth_i[1]).item(),torch.std(depth_i[2]).item()])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":219,"status":"ok","timestamp":1670983842288,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"},"user_tz":300},"id":"U_rfbETBWUdj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"96deb519-1514-4207-b2e2-a3f7003aee5e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 224, 224])"]},"metadata":{},"execution_count":23}],"source":["depth_i.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mq-01KXmYKlW"},"outputs":[],"source":["means"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMzZUuOSWWed"},"outputs":[],"source":["norm_mean=np.mean(means,axis =0)\n","norm_mean = list(norm_mean)\n","norm_mean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SyTU6d88WYMW"},"outputs":[],"source":["norm_std=np.mean(stds,axis=0)\n","norm_std = list(norm_std)\n","norm_std"]},{"cell_type":"markdown","metadata":{"id":"Vq7EpxYh083G"},"source":["# Train Dataset & Train loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tw9RHT8WxqBB"},"outputs":[],"source":["#transform=transforms.Compose([transforms.ToTensor()])\n","train_dataset = LazyLoadDataset(\"/content/\",transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize(norm_mean, norm_std),])) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ca3yNB4xw0q"},"outputs":[],"source":["train_dataset[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnZKPKFexwuZ"},"outputs":[],"source":["(depth0, field_id), Y = train_dataset[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YhtDmDf_paWa"},"outputs":[],"source":["batch_size_train = 4\n","train_loader=DataLoader(train_dataset,batch_size=batch_size_train,shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MTPgo2hH9Bev"},"outputs":[],"source":["for batch_idx, (data, target) in enumerate(train_loader):\n","  #print(data) #plot the channel 3 of img0 in one batch\n","  print(data[0].shape)\n","  plt.imshow(data[0][0][0])\n","  #plt.show()\n","  #plt.imshow(data[0])\n","  break"]},{"cell_type":"markdown","metadata":{"id":"dBLxVxhh1Avv"},"source":["# Test Dataset & Test loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxOFElBkyHIx"},"outputs":[],"source":["test_dataset = LazyLoadDataset(\"/content/\",train=False,transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize(norm_mean, norm_std),\n","                   ])) #,transform=transform\n","\n","batch_size_test = 2\n","test_loader=DataLoader(test_dataset,batch_size=batch_size_test,shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"sz59fZCVk7JU"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vY_bCsYCkLME"},"outputs":[],"source":["class Res(nn.Module):\n","    def __init__(self, input_channels,output_size):\n","        super(Res, self).__init__()\n","        \n","        # We use Sequential for simplicity\n","        self.stack = nn.Sequential(models.resnet50(pretrained=True),\n","                                   nn.Linear(1000, output_size))\n","                                 \n","    def forward(self, x):\n","        x = self.stack(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8jA6M29ol2Hu"},"outputs":[],"source":["# Model input/output settings\n","input_channels = 3 # number of input channels\n","output_size=12"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0_gWSqmkTQe"},"outputs":[],"source":["def train(epoch, model, optimizer,loader=train_loader):\n","    \"\"\"\n","    Train the model for one epoch\n","\n","    Args:\n","        epoch (int): current epoch\n","        model (nn.Module): model to train\n","        optimizer (torch.optim): optimizer to use\n","        loader: val/train loader\n","    \"\"\"\n","    model.train()\n","    for batch_idx,((data, id),target) in enumerate(loader):\n","        # send to device\n","        data, target = data.to(device), target.to(device)\n","        data = data.view(len(id),3,224, 224)\n","\n","        # consider passing different data augmentation in training\n","        # data = F.invert()\n","        # data = F.adjust_sharpness(data,sharpness_factor = 0.5)\n","        # data = F.adjust_hue(data,hue_factor = 0.9)\n","        # data = F.adjust_saturation(data,saturation_factor = 0.5)\n","\n","        # make sure we erase all the gradients before computing new ones\n","        optimizer.zero_grad() \n","        \n","        # forward propagation\n","        output = model(data)\n","\n","        #RMSE loss\n","        loss = torch.sqrt(F.mse_loss(output,target))\n","        \n","        # backward propagation\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if batch_idx % 200 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(loader.dataset),\n","                100. * batch_idx / len(loader), loss))\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eIReW5i30pDu"},"outputs":[],"source":["def test(model):\n","    \"\"\"\n","    Test the model\n","\n","    Args:\n","        model (nn.Module): model to test\n","      \n","    \"\"\"\n","    model.eval()\n","    ids = []\n","    preds = []\n","    with torch.no_grad():\n","      for batch_idx,(data, id) in enumerate(test_loader):\n","        # send to device\n","        data = data.to(device)          \n","        pred = model(data)\n","        for i in range(len(id)):\n","          ids.append(id[i])\n","          preds.append(np.array(pred[i].cpu()/1000,dtype=\"float64\"))\n","\n","    return ids, preds  \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3c03-3DfB1Z"},"outputs":[],"source":["def get_n_params(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"1UerPOG-mIjr"},"source":["# Cross-Validation & Grid-Search for determining hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"iQdzvsW8mLOt"},"source":["What should hyperparameters in the models be? We could perform grid-search on the whole train dataset and pick the best-performing hyerparameters. But to avoid over-fitting on the train dataset, we split the whole train dataset into 9:1.\n","*   cross_val_train_set (90% of train dataset)\n","*   cross_val_test_set (10% of train dataset)\n","\n","We train our model on the cross_val_train_set and test our model on cross_val_test_set. We pick the best-performing hyperparameters on the cross_val_test_set.\n","\n","After all these steps, we train our model using the picked hyperparameters on the whole train dataset. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTqKNtgQmMmC"},"outputs":[],"source":["cross_val_train_set, cross_val_test_set = train_test_split(train_dataset, test_size=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJR5sxl-mM4d"},"outputs":[],"source":["# Sanity Check\n","len(cross_val_train_set) == int(len(train_dataset)*0.9)\n","len(cross_val_train_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ghv0DrRXmM_h"},"outputs":[],"source":["batch_size_cross_val_train = 4\n","batch_size_cross_val_test = len(cross_val_test_set) # = 340\n","len(cross_val_test_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPZ3gosjmNKc"},"outputs":[],"source":["cross_val_train_loader=DataLoader(cross_val_train_set,batch_size=batch_size_cross_val_train,shuffle=True)\n","cross_val_test_loader=DataLoader(cross_val_test_set,batch_size=batch_size_cross_val_test,shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3VrVFZXFmdLL"},"outputs":[],"source":["len(cross_val_test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vY8lohJ2mfWy"},"outputs":[],"source":["def cross_val(model):\n","    \"\"\"\n","    Test the model\n","\n","    Args:\n","        model (nn.Module): model to test\n","      \n","    \"\"\"\n","    model.eval()\n","    # We can just edit on the code for train(). We should delete the bakward propagation part and optimizer part.\n","\n","    with torch.no_grad():\n","      for batch_idx,((data, id),target) in enumerate(cross_val_test_loader):\n","          # send to device\n","          if batch_idx==0:\n","            data, target = data.to(device), target.to(device)\n","            \n","            # forward propagation\n","            output = model(data)\n","\n","            #RMSE loss\n","            loss = torch.sqrt(F.mse_loss(output,target)) \n","            return loss.item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gb4CaUMymhrv"},"outputs":[],"source":["def SGD_hyper_tuning(lr_list,momentum_list):  \n","  for j in momentum_list:\n","    for i in lr_list:\n","      model_res = Res(input_channels, output_size) # create Res model\n","      model_res.to(device)\n","      print(\"lr=\",i,\"momentum=\",j)\n","      train(epoch=0, model=model_res, optimizer=torch.optim.SGD(model_res.parameters(), lr=i,momentum=j),loader=cross_val_train_loader)\n","      print(\"Validation Performance:\",cross_val(model_res))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-KdHHMbmj1_"},"outputs":[],"source":["lr_list = [1e-5,1e-4,1e-3,1e-2,1e-1]\n","momentum_list = [0.99,0.97,0.95,0.93,0.91]\n","SGD_hyper_tuning(lr_list,momentum_list)"]},{"cell_type":"markdown","metadata":{"id":"BJDbx5WOnUYO"},"source":["Top 3 Performance\n","1.  lr = 1e-3, momentum = 0.93, loss on validation set = 6.5\n","2.  lr = 1e-3, momentum = 0.91, loss on validation set = 7.3\n","3.  lr = 1e-3, momentum = 0.95, loss on validation set = 7.5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A2vgsCTjn8t-"},"outputs":[],"source":["lr_list = [8e-4,9e-4,1e-3,2e-3,3e-3]\n","momentum_list = [0.99,0.97,0.95,0.93,0.91]\n","SGD_hyper_tuning(lr_list,momentum_list)"]},{"cell_type":"markdown","metadata":{"id":"tEmVPclktF2G"},"source":["Top 5 Performance\n","\n","1.   lr = 8e-4, momentum = 0.91, loss on validation set = 6.09\n","2.   lr = 1e-3, momentum = 0.93, loss on validation set = 6.45\n","3.   lr = 1e-3, momentum = 0.91, loss on validation set = 6.52\n","4.   lr = 9e-4, momentum = 0.93, loss on validation set = 6.99\n","5.   lr = 8e-4, momentum = 0.95, loss on validation set = 7.34"]},{"cell_type":"markdown","metadata":{"id":"Gfs-W5LFxtct"},"source":["# lr=8e-4, momentum=0.91, 20 epoch, train_batch_size = 4, no lr decay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GjOWLaNvxhMm"},"outputs":[],"source":["model_res = Res(input_channels, output_size) # create Res model\n","model_res.to(device)\n","optimizer = torch.optim.SGD(model_res.parameters(), lr=8e-4,momentum=0.91) \n","\n","print(\"lr=\",8e-4,\"momentum=\",0.91)\n","for epoch in range(20):\n","  train(epoch, model=model_res, optimizer=torch.optim.SGD(model_res.parameters(), lr=8e-4,momentum=0.91),loader=train_loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DB3647mMyJbh"},"outputs":[],"source":["torch.save(model_res,'/content/drive/MyDrive/ML_Capstone/project_v8_res_3depth_00008091_nodecay_20ep.pt')\n","model_res = torch.load('/content/drive/MyDrive/ML_Capstone/project_v8_res_3depth_00008091_nodecay_20ep.pt')\n","model_res.eval()\n","ids,preds = test(model_res)\n","\n","outfile = '/content/drive/MyDrive/ML_Capstone/submission_v8_00008091_nodecay_20ep.csv'\n","\n","output_file = open(outfile, 'w')\n","\n","titles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n","         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\n","\n","df = pd.concat([pd.DataFrame(ids), pd.DataFrame.from_records(preds)], axis = 1, names = titles)\n","df.columns = titles\n","df.to_csv(outfile, index = False)\n","print(\"Written to csv file {}\".format(outfile))"]},{"cell_type":"markdown","metadata":{"id":"pcnHUTlI0F_1"},"source":["# lr=1e-3, momentum=0.93, 20 epoch, train_batch_size = 4, no lr decay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ocCrcGo0Pn_"},"outputs":[],"source":["model_res = Res(input_channels, output_size) # create Res model\n","model_res.to(device)\n","optimizer = torch.optim.SGD(model_res.parameters(), lr=1e-3,momentum=0.93) \n","\n","print(\"lr=\",1e-3,\"momentum=\",0.93)\n","for epoch in range(20):\n","  train(epoch, model=model_res, optimizer=torch.optim.SGD(model_res.parameters(), lr=1e-3,momentum=0.93),loader=train_loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9BEZDmcd0R5E"},"outputs":[],"source":["torch.save(model_res,'/content/drive/MyDrive/ML_Capstone/project_v8_res_3depth_0001093_nodecay_20ep.pt')\n","model_res = torch.load('/content/drive/MyDrive/ML_Capstone/project_v8_res_3depth_0001093_nodecay_20ep.pt')\n","model_res.eval()\n","ids,preds = test(model_res)\n","\n","outfile = '/content/drive/MyDrive/ML_Capstone/submission_v8_0001093_nodecay_20ep.csv'\n","\n","output_file = open(outfile, 'w')\n","\n","titles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n","         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\n","\n","df = pd.concat([pd.DataFrame(ids), pd.DataFrame.from_records(preds)], axis = 1, names = titles)\n","df.columns = titles\n","df.to_csv(outfile, index = False)\n","print(\"Written to csv file {}\".format(outfile))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"28TGhoQ34Dag"},"outputs":[],"source":["for:\n","model_res = Res(input_channels, output_size) # create Res model\n","model_res.to(device)\n","optimizer = torch.optim.SGD(model_res.parameters(), lr=5*1e-3,momentum=0.92) # use SGD with learning rate 0.01 and momentum 0.5\n","print('Number of parameters: {}'.format(get_n_params(model_res)))\n","\n","test_pred = []\n","for epoch in range(0, 1):\n","    train(epoch, model_res, optimizer)\n","    test_pred.append(test(model_res))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qcmGapI000a7"},"outputs":[],"source":["torch.save(model_res,'project_v8_res_3depth.pt')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1JIcwNPq7fKju7p_5tRRsL1lcEwpVJwSG","timestamp":1670369823694},{"file_id":"15o2ZsaEe9cUqMbfAPUJ6d0Ijaep-fnMc","timestamp":1670359281346},{"file_id":"1FPCPdpJkoZz07m90YO5OBanS5xN9BYJc","timestamp":1670203867975}],"collapsed_sections":["dBLxVxhh1Avv","1UerPOG-mIjr","Gfs-W5LFxtct","pcnHUTlI0F_1"]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}