{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1dOxS0n-LenAsTFynczmWP6jZ-TVeTuew","timestamp":1671061735534}],"collapsed_sections":["dBLxVxhh1Avv"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive/\",force_remount=True)\n","\n","from zipfile import ZipFile\n","! cp '/content/drive/MyDrive/ML_Capstone/drive-download.zip' '/content'\n","from zipfile import ZipFile\n","zip = ZipFile('/content/drive-download.zip')\n","zip.extractall()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hSh-vsGQwAza","outputId":"aed48591-f6c0-430c-b2bd-03916f6067d3","executionInfo":{"status":"ok","timestamp":1670887460629,"user_tz":300,"elapsed":40024,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["# importing required packages\n","import os\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from torchvision.transforms import ToTensor\n","import numpy as np\n","import cv2\n","import pickle as pkl\n","import matplotlib.pyplot as plt\n","from torchvision import models\n","from PIL import Image\n","from torch.optim.lr_scheduler import StepLR\n","from sklearn.model_selection import train_test_split\n","import pandas as pd"],"metadata":{"id":"njbLHwZV0vHq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set device to cuda if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"id":"DEYD2JLU5izs","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"590e8119-04cb-4352-dde1-3a84c1dbf332","executionInfo":{"status":"ok","timestamp":1670887465890,"user_tz":300,"elapsed":679,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# Dataset and Dataloaders"],"metadata":{"id":"oD3nH-6i1vsg"}},{"cell_type":"markdown","source":["# Lazy Loading"],"metadata":{"id":"180ylhvU_sTl"}},{"cell_type":"markdown","source":["data\n","-train\n","  - X\n","    - 1\n","      - rgb\n","        - 0.png\n","        - 1.png\n","        - 2.png\n","      - depth.npy\n","      - field_id.pkl\n","    \n","    - 2\n","\n","    - ...\n","  - Y\n","    - 1.npy\n","    - 2.npy\n","\n","-test"],"metadata":{"id":"HdBnI6RKBBLD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Kjw2im8zrug"},"outputs":[],"source":["class LazyLoadDataset(Dataset):\n","  def __init__(self,path,train=True,transform=None):\n","    self.train = train\n","    self.transform = transform\n","    path = path + (\"train/\" if self.train else \"test/\")\n","\n","    self.pathX = path + \"X/\"\n","    self.pathY = path + \"Y/\"\n","\n","    self.data = os.listdir(self.pathX)\n","\n","  def __getitem__(self,idx):\n","    f = self.data[idx]\n","\n","    # X\n","    # read rgb images\n","    img0=Image.open(self.pathX + f + \"/rgb/0.png\")\n","\n","\n","    if self.transform is not None:\n","      img0 = self.transform(img0)\n","\n","    # read field ID\n","    field_id = pkl.load(open(self.pathX + f + \"/field_id.pkl\",\"rb\"))\n","\n","    if self.train==True:\n","      # Y \n","      Y = np.load(self.pathY + f + \".npy\")*1000\n","      return (img0.float().type(torch.float32),field_id),torch.from_numpy(Y).float().type(torch.float32)\n","\n","    else: # if test, there is no Y\n","      return img0.float().type(torch.float32),field_id\n","\n","  def __len__(self):\n","    return len(self.data)\n"]},{"cell_type":"markdown","source":["# Normalization: Compute Mean and Std\n","\n"],"metadata":{"id":"stlacrCtoQjh"}},{"cell_type":"code","source":["train_dataset_raw = LazyLoadDataset(\"/content/\",train=True,transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       #transforms.RandomRotation(degrees=(0, 30)),\n","                       #transforms.ColorJitter(brightness=0.5, contrast=1, saturation=0.1, hue=0.5)\n","                       #transforms.RandomPerspective(distortion_scale=0.5, p=0.4)                       \n","                   ])) "],"metadata":{"id":"HscU7H-cWAnS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(img_i, id_i),Y_i=train_dataset_raw[0]\n","img_i.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vArFokH3NtDB","outputId":"2c57f2ba-f569-4fc6-8166-ab23017da79d","executionInfo":{"status":"ok","timestamp":1670887465890,"user_tz":300,"elapsed":7,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 224, 224])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["means=[]\n","stds=[]\n","for i in range(3396):\n","  (img_i, id_i),Y_i=train_dataset_raw[i]\n","  means.append([torch.mean(img_i[0]).item(),torch.mean(img_i[1]).item(),torch.mean(img_i[2]).item()])\n","  stds.append([torch.std(img_i[0]).item(),torch.std(img_i[1]).item(),torch.std(img_i[2]).item()])"],"metadata":{"id":"YZQJz4jqNrIR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_i"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U_rfbETBWUdj","outputId":"2d9c7ffd-492d-4e0c-a724-9539db4384fd","executionInfo":{"status":"ok","timestamp":1670887478438,"user_tz":300,"elapsed":34,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.5804, 0.5804, 0.5804,  ..., 0.4353, 0.4431, 0.4510],\n","         [0.5843, 0.5843, 0.5843,  ..., 0.4392, 0.4510, 0.4510],\n","         [0.6000, 0.5922, 0.5804,  ..., 0.4353, 0.4431, 0.4431],\n","         ...,\n","         [0.0706, 0.0667, 0.0549,  ..., 0.4824, 0.4902, 0.4824],\n","         [0.0824, 0.0745, 0.0588,  ..., 0.4824, 0.4824, 0.4824],\n","         [0.0824, 0.0627, 0.0588,  ..., 0.4824, 0.4745, 0.4784]],\n","\n","        [[0.6000, 0.6078, 0.6078,  ..., 0.4353, 0.4431, 0.4510],\n","         [0.6039, 0.6039, 0.6039,  ..., 0.4392, 0.4471, 0.4471],\n","         [0.6196, 0.6118, 0.6000,  ..., 0.4392, 0.4431, 0.4431],\n","         ...,\n","         [0.1020, 0.0980, 0.0863,  ..., 0.4824, 0.4902, 0.4824],\n","         [0.1059, 0.0980, 0.0824,  ..., 0.4941, 0.4941, 0.4941],\n","         [0.1059, 0.0863, 0.0824,  ..., 0.4941, 0.4863, 0.4902]],\n","\n","        [[0.6275, 0.6314, 0.6314,  ..., 0.4118, 0.4196, 0.4353],\n","         [0.6314, 0.6314, 0.6314,  ..., 0.4235, 0.4471, 0.4471],\n","         [0.6471, 0.6392, 0.6275,  ..., 0.4235, 0.4275, 0.4275],\n","         ...,\n","         [0.0902, 0.0941, 0.0824,  ..., 0.4824, 0.4902, 0.4824],\n","         [0.0980, 0.0980, 0.0824,  ..., 0.4980, 0.4902, 0.4902],\n","         [0.0980, 0.0863, 0.0824,  ..., 0.4902, 0.4824, 0.4863]]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["means"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mq-01KXmYKlW","outputId":"b986fb58-751b-4acb-9b3e-3841f993ab84","executionInfo":{"status":"ok","timestamp":1670887478438,"user_tz":300,"elapsed":32,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0.4089660942554474, 0.42159706354141235, 0.4235861897468567],\n"," [0.3838418126106262, 0.42807045578956604, 0.43631651997566223],\n"," [0.4239005148410797, 0.43631479144096375, 0.43936529755592346],\n"," [0.42188531160354614, 0.4352859854698181, 0.4394019842147827],\n"," [0.4140632450580597, 0.4274289906024933, 0.4306897521018982],\n"," [0.40640950202941895, 0.42009836435317993, 0.423117071390152],\n"," [0.39520496129989624, 0.4208911061286926, 0.4490075409412384],\n"," [0.3936443626880646, 0.4017176628112793, 0.4171583354473114],\n"," [0.4165172278881073, 0.42737245559692383, 0.42841607332229614],\n"," [0.4034387171268463, 0.40254780650138855, 0.41425931453704834],\n"," [0.42331552505493164, 0.4354636073112488, 0.43742889165878296],\n"," [0.3891490697860718, 0.4226757884025574, 0.4550773501396179],\n"," [0.39972662925720215, 0.41498345136642456, 0.41894352436065674],\n"," [0.40556880831718445, 0.40412941575050354, 0.4156002104282379],\n"," [0.40395215153694153, 0.4053763449192047, 0.4292147159576416],\n"," [0.3904187083244324, 0.4267692267894745, 0.45910316705703735],\n"," [0.39531469345092773, 0.4330909848213196, 0.4576168358325958],\n"," [0.40739938616752625, 0.4093925356864929, 0.4356677830219269],\n"," [0.38521677255630493, 0.42894551157951355, 0.4367542266845703],\n"," [0.3881567716598511, 0.4144657552242279, 0.44283583760261536],\n"," [0.38138049840927124, 0.41870781779289246, 0.45064276456832886],\n"," [0.40771251916885376, 0.44784852862358093, 0.45772793889045715],\n"," [0.3892345428466797, 0.4232466220855713, 0.452306866645813],\n"," [0.38124626874923706, 0.4060814082622528, 0.4369029998779297],\n"," [0.39765769243240356, 0.44246751070022583, 0.45199596881866455],\n"," [0.3849722445011139, 0.4064083397388458, 0.4233279228210449],\n"," [0.3972208499908447, 0.4102593958377838, 0.4127845764160156],\n"," [0.39118844270706177, 0.4262219965457916, 0.45657604932785034],\n"," [0.3926675617694855, 0.41547709703445435, 0.4341649115085602],\n"," [0.41650527715682983, 0.4129316508769989, 0.4264928996562958],\n"," [0.39220187067985535, 0.40083780884742737, 0.4159679710865021],\n"," [0.3890678584575653, 0.42411357164382935, 0.4550701975822449],\n"," [0.4190598428249359, 0.4171907603740692, 0.4294184446334839],\n"," [0.3725537657737732, 0.4174114465713501, 0.42563310265541077],\n"," [0.3943535089492798, 0.3949378728866577, 0.41819724440574646],\n"," [0.4051463007926941, 0.4183255732059479, 0.4208598732948303],\n"," [0.3963198661804199, 0.4100137948989868, 0.4129786491394043],\n"," [0.39684629440307617, 0.3967226445674896, 0.4088749885559082],\n"," [0.37448135018348694, 0.41138574481010437, 0.44368723034858704],\n"," [0.3869265019893646, 0.4132521152496338, 0.4413888156414032],\n"," [0.40202200412750244, 0.41460371017456055, 0.41930633783340454],\n"," [0.36847996711730957, 0.4051477909088135, 0.4373409152030945],\n"," [0.4178239405155182, 0.41703805327415466, 0.42968854308128357],\n"," [0.41105082631111145, 0.4222428798675537, 0.42340517044067383],\n"," [0.41084107756614685, 0.4251163899898529, 0.4287034571170807],\n"," [0.3760797083377838, 0.42159903049468994, 0.43104690313339233],\n"," [0.392438143491745, 0.4157733619213104, 0.4347560405731201],\n"," [0.4066765010356903, 0.42851385474205017, 0.43258440494537354],\n"," [0.39445918798446655, 0.43808838725090027, 0.4460517466068268],\n"," [0.4318508505821228, 0.4438399374485016, 0.4466991126537323],\n"," [0.4117544889450073, 0.41673991084098816, 0.44029781222343445],\n"," [0.40863287448883057, 0.42459383606910706, 0.4295710325241089],\n"," [0.4105684757232666, 0.41223350167274475, 0.4358025789260864],\n"," [0.4015740752220154, 0.4211153984069824, 0.4374493658542633],\n"," [0.37713027000427246, 0.41213491559028625, 0.44638001918792725],\n"," [0.4087778627872467, 0.4261898398399353, 0.4356193244457245],\n"," [0.379961758852005, 0.42583340406417847, 0.43531110882759094],\n"," [0.3979713022708893, 0.42305031418800354, 0.4496827721595764],\n"," [0.3817363679409027, 0.40912365913391113, 0.4364769458770752],\n"," [0.3774051368236542, 0.4204288721084595, 0.4273986518383026],\n"," [0.4003266394138336, 0.40403059124946594, 0.43141767382621765],\n"," [0.37426429986953735, 0.40914100408554077, 0.43893709778785706],\n"," [0.37769633531570435, 0.40618959069252014, 0.43418386578559875],\n"," [0.40930622816085815, 0.4083605110645294, 0.4204846918582916],\n"," [0.38436657190322876, 0.4201134741306305, 0.45218750834465027],\n"," [0.411945641040802, 0.42667049169540405, 0.4311847984790802],\n"," [0.41986146569252014, 0.41840434074401855, 0.4301329553127289],\n"," [0.4070836305618286, 0.41890740394592285, 0.42101985216140747],\n"," [0.37944746017456055, 0.3887048363685608, 0.4049101769924164],\n"," [0.4156683385372162, 0.41719046235084534, 0.44317108392715454],\n"," [0.4171065390110016, 0.41647160053253174, 0.4409623444080353],\n"," [0.39527130126953125, 0.43101269006729126, 0.4624924063682556],\n"," [0.4200206696987152, 0.43244072794914246, 0.4372169077396393],\n"," [0.4226067066192627, 0.4231218695640564, 0.44593146443367004],\n"," [0.3994167447090149, 0.4007793366909027, 0.4256250560283661],\n"," [0.3772960305213928, 0.4108785092830658, 0.4368904232978821],\n"," [0.42073145508766174, 0.43365779519081116, 0.43692129850387573],\n"," [0.370941698551178, 0.40835267305374146, 0.4402509331703186],\n"," [0.39289310574531555, 0.42890286445617676, 0.4597383737564087],\n"," [0.38998451828956604, 0.4247099757194519, 0.45454955101013184],\n"," [0.3879583179950714, 0.4234754741191864, 0.4548759460449219],\n"," [0.3863518238067627, 0.42148807644844055, 0.45230787992477417],\n"," [0.37398070096969604, 0.41016095876693726, 0.4422146677970886],\n"," [0.36569342017173767, 0.4036730229854584, 0.4357284605503082],\n"," [0.3854665160179138, 0.38781723380088806, 0.4122198522090912],\n"," [0.4005758762359619, 0.4160892367362976, 0.42113256454467773],\n"," [0.38790231943130493, 0.41218888759613037, 0.4420471489429474],\n"," [0.378570020198822, 0.4152206480503082, 0.44829061627388],\n"," [0.3823768198490143, 0.40700212121009827, 0.43741902709007263],\n"," [0.35596489906311035, 0.38224565982818604, 0.41302618384361267],\n"," [0.362911581993103, 0.38814935088157654, 0.41858887672424316],\n"," [0.3868729770183563, 0.43005040287971497, 0.4377150237560272],\n"," [0.41522568464279175, 0.42730051279067993, 0.42957162857055664],\n"," [0.41453489661216736, 0.42686623334884644, 0.4300558865070343],\n"," [0.3821258544921875, 0.41881832480430603, 0.4513561725616455],\n"," [0.40487515926361084, 0.4177176058292389, 0.4197896718978882],\n"," [0.4062909781932831, 0.40871191024780273, 0.43556809425354004],\n"," [0.40513259172439575, 0.40493547916412354, 0.4275015592575073],\n"," [0.3842141926288605, 0.40907397866249084, 0.43771931529045105],\n"," [0.39929530024528503, 0.4132929742336273, 0.41735297441482544],\n"," [0.3735964894294739, 0.39979425072669983, 0.4270174205303192],\n"," [0.3916591703891754, 0.40687933564186096, 0.4101545810699463],\n"," [0.3992299437522888, 0.42505863308906555, 0.4528280794620514],\n"," [0.39769673347473145, 0.4091843068599701, 0.41291430592536926],\n"," [0.39302974939346313, 0.4196151793003082, 0.447981595993042],\n"," [0.3834879696369171, 0.42068567872047424, 0.4519340991973877],\n"," [0.40082499384880066, 0.40677568316459656, 0.42030736804008484],\n"," [0.3829484283924103, 0.4190555512905121, 0.4516173005104065],\n"," [0.40390920639038086, 0.4169643223285675, 0.41953930258750916],\n"," [0.4126531183719635, 0.42401349544525146, 0.42556968331336975],\n"," [0.3878854513168335, 0.43244531750679016, 0.44163960218429565],\n"," [0.4143974184989929, 0.45041072368621826, 0.4559701085090637],\n"," [0.3858487904071808, 0.41326743364334106, 0.44029977917671204],\n"," [0.38365045189857483, 0.42551013827323914, 0.436215341091156],\n"," [0.40369975566864014, 0.4185848534107208, 0.42337286472320557],\n"," [0.399170458316803, 0.4124903082847595, 0.41520220041275024],\n"," [0.38695618510246277, 0.4130931496620178, 0.4403480589389801],\n"," [0.38802602887153625, 0.42308175563812256, 0.45540136098861694],\n"," [0.3691106140613556, 0.399045467376709, 0.4272705316543579],\n"," [0.4205993711948395, 0.4341990351676941, 0.43805795907974243],\n"," [0.4010985195636749, 0.4150804281234741, 0.41826170682907104],\n"," [0.3807734549045563, 0.40813687443733215, 0.4361906945705414],\n"," [0.3685290813446045, 0.40542030334472656, 0.43792423605918884],\n"," [0.3919544517993927, 0.41315799951553345, 0.4304642081260681],\n"," [0.4251076877117157, 0.4398818910121918, 0.4448731541633606],\n"," [0.3828532099723816, 0.4178304970264435, 0.450320303440094],\n"," [0.41881421208381653, 0.4181533753871918, 0.4411109983921051],\n"," [0.3992583751678467, 0.4262979030609131, 0.45172345638275146],\n"," [0.4261459708213806, 0.42479485273361206, 0.44808706641197205],\n"," [0.422155499458313, 0.4345379173755646, 0.4409477114677429],\n"," [0.3960338830947876, 0.41871556639671326, 0.43726715445518494],\n"," [0.3913276493549347, 0.4019224941730499, 0.41760823130607605],\n"," [0.4075227975845337, 0.41948166489601135, 0.42163947224617004],\n"," [0.3931550085544586, 0.4283548593521118, 0.4588179290294647],\n"," [0.43157386779785156, 0.431641161441803, 0.4565485417842865],\n"," [0.39997225999832153, 0.4017932415008545, 0.4259886145591736],\n"," [0.3797437250614166, 0.40649741888046265, 0.4339461922645569],\n"," [0.4203650951385498, 0.4284595251083374, 0.43566352128982544],\n"," [0.3893507421016693, 0.42362454533576965, 0.4536705017089844],\n"," [0.4097929894924164, 0.41096028685569763, 0.42487460374832153],\n"," [0.39446592330932617, 0.42473873496055603, 0.4509660303592682],\n"," [0.37247440218925476, 0.4006045162677765, 0.42938312888145447],\n"," [0.3700486421585083, 0.40691378712654114, 0.4375627934932709],\n"," [0.3767317831516266, 0.4103618264198303, 0.4401871860027313],\n"," [0.3937579095363617, 0.4274623692035675, 0.4608837366104126],\n"," [0.3788057565689087, 0.42293286323547363, 0.4324605464935303],\n"," [0.414590984582901, 0.4232597351074219, 0.42802608013153076],\n"," [0.39526909589767456, 0.44002407789230347, 0.44885003566741943],\n"," [0.41032370924949646, 0.4250658452510834, 0.42842522263526917],\n"," [0.3999994993209839, 0.4150839149951935, 0.4181620478630066],\n"," [0.40620100498199463, 0.419003427028656, 0.4211193919181824],\n"," [0.3578936457633972, 0.40415245294570923, 0.4144326150417328],\n"," [0.38090670108795166, 0.4087938666343689, 0.4369279146194458],\n"," [0.3741922974586487, 0.41149067878723145, 0.44240304827690125],\n"," [0.3920559883117676, 0.40034228563308716, 0.4148518741130829],\n"," [0.4065769612789154, 0.4076700806617737, 0.42131754755973816],\n"," [0.3867054879665375, 0.4236412048339844, 0.4525820314884186],\n"," [0.4067856967449188, 0.42441558837890625, 0.4342656135559082],\n"," [0.3847348093986511, 0.42830145359039307, 0.4364855885505676],\n"," [0.3983595073223114, 0.4215080142021179, 0.45078569650650024],\n"," [0.41086339950561523, 0.4240477383136749, 0.4263496398925781],\n"," [0.3719939887523651, 0.4093799889087677, 0.44003793597221375],\n"," [0.4035606384277344, 0.4003317058086395, 0.4139525294303894],\n"," [0.37251636385917664, 0.41687384247779846, 0.42648836970329285],\n"," [0.4033489227294922, 0.40208637714385986, 0.4137437045574188],\n"," [0.41008061170578003, 0.4247051179409027, 0.42767077684402466],\n"," [0.3941008448600769, 0.4306459128856659, 0.46327412128448486],\n"," [0.3800058960914612, 0.41719919443130493, 0.4499072730541229],\n"," [0.4118505120277405, 0.4249941110610962, 0.42800670862197876],\n"," [0.37424948811531067, 0.4123893976211548, 0.44600987434387207],\n"," [0.3824975788593292, 0.41817641258239746, 0.4502076208591461],\n"," [0.41607192158699036, 0.4285085201263428, 0.4316985607147217],\n"," [0.3696388006210327, 0.4161422848701477, 0.42587926983833313],\n"," [0.385811984539032, 0.3938309848308563, 0.40824049711227417],\n"," [0.42220890522003174, 0.42164671421051025, 0.4448338449001312],\n"," [0.3658037781715393, 0.3955579400062561, 0.42433586716651917],\n"," [0.376833975315094, 0.4019864797592163, 0.4283217191696167],\n"," [0.3912461996078491, 0.4174937903881073, 0.44526734948158264],\n"," [0.4081394672393799, 0.4061991274356842, 0.41639387607574463],\n"," [0.37627121806144714, 0.40015992522239685, 0.42951300740242004],\n"," [0.3888515532016754, 0.4297860860824585, 0.45985350012779236],\n"," [0.3804773688316345, 0.38991180062294006, 0.40585821866989136],\n"," [0.3723942041397095, 0.38227564096450806, 0.3989865183830261],\n"," [0.3978094458580017, 0.41218817234039307, 0.4152457118034363],\n"," [0.3902772068977356, 0.39851465821266174, 0.4133225381374359],\n"," [0.4142017364501953, 0.4291951358318329, 0.43395712971687317],\n"," [0.4180404543876648, 0.41758638620376587, 0.4410310983657837],\n"," [0.39128199219703674, 0.4169689118862152, 0.44553568959236145],\n"," [0.4208507835865021, 0.4209442436695099, 0.44551074504852295],\n"," [0.37705284357070923, 0.41310399770736694, 0.446564257144928],\n"," [0.41672271490097046, 0.4167932868003845, 0.4293373227119446],\n"," [0.41440123319625854, 0.42807576060295105, 0.4308924078941345],\n"," [0.40666162967681885, 0.40627431869506836, 0.4290170967578888],\n"," [0.3816664516925812, 0.4196036159992218, 0.4534665644168854],\n"," [0.3832078278064728, 0.4182467460632324, 0.45028215646743774],\n"," [0.3939473628997803, 0.4275411069393158, 0.45736709237098694],\n"," [0.4160008728504181, 0.4182376563549042, 0.4431597590446472],\n"," [0.40734657645225525, 0.4207010865211487, 0.42234382033348083],\n"," [0.3885415494441986, 0.4330929219722748, 0.4423365592956543],\n"," [0.3737608790397644, 0.4099844694137573, 0.440738320350647],\n"," [0.3680763244628906, 0.4148564636707306, 0.4250760078430176],\n"," [0.388986200094223, 0.41360610723495483, 0.4423723816871643],\n"," [0.4184967279434204, 0.43099477887153625, 0.4337860345840454],\n"," [0.37703651189804077, 0.4234143793582916, 0.4332549571990967],\n"," [0.417022168636322, 0.42998361587524414, 0.43259966373443604],\n"," [0.3838297426700592, 0.3917951285839081, 0.4067372679710388],\n"," [0.391037255525589, 0.4170432984828949, 0.4445950388908386],\n"," [0.3854220509529114, 0.4147343635559082, 0.4437011480331421],\n"," [0.42678743600845337, 0.44020435214042664, 0.443912148475647],\n"," [0.40360575914382935, 0.41780993342399597, 0.42125630378723145],\n"," [0.38258492946624756, 0.4074513614177704, 0.43748384714126587],\n"," [0.3726035952568054, 0.4012582004070282, 0.4290436804294586],\n"," [0.41849228739738464, 0.4186042845249176, 0.44209349155426025],\n"," [0.39119818806648254, 0.4362112283706665, 0.4455050230026245],\n"," [0.40929678082466125, 0.4101056158542633, 0.4338371157646179],\n"," [0.39274314045906067, 0.42955464124679565, 0.46134722232818604],\n"," [0.3880903422832489, 0.4245310127735138, 0.45675158500671387],\n"," [0.41886937618255615, 0.4209001064300537, 0.44397440552711487],\n"," [0.3875797688961029, 0.4110793173313141, 0.4355986714363098],\n"," [0.3880256414413452, 0.4335446357727051, 0.44352594017982483],\n"," [0.37943702936172485, 0.4152102470397949, 0.43810370564460754],\n"," [0.374153733253479, 0.41844481229782104, 0.42825567722320557],\n"," [0.38870683312416077, 0.4320617914199829, 0.4396112263202667],\n"," [0.390857994556427, 0.4281735420227051, 0.4611908197402954],\n"," [0.4012312889099121, 0.4022105038166046, 0.4267440736293793],\n"," [0.3667297661304474, 0.39407795667648315, 0.42330387234687805],\n"," [0.39222168922424316, 0.42280110716819763, 0.4551968574523926],\n"," [0.41080960631370544, 0.41280442476272583, 0.43479686975479126],\n"," [0.4068552255630493, 0.4219059944152832, 0.4262411892414093],\n"," [0.4007091224193573, 0.4163425862789154, 0.42128652334213257],\n"," [0.39001211524009705, 0.3922014534473419, 0.41646474599838257],\n"," [0.41708263754844666, 0.4177486002445221, 0.43562644720077515],\n"," [0.4262741208076477, 0.4255840480327606, 0.44993388652801514],\n"," [0.43499988317489624, 0.4338933527469635, 0.45825377106666565],\n"," [0.39475229382514954, 0.3972131013870239, 0.4240621328353882],\n"," [0.4186328947544098, 0.41864892840385437, 0.44276925921440125],\n"," [0.37736040353775024, 0.419025719165802, 0.4252989590167999],\n"," [0.3895663917064667, 0.41387978196144104, 0.44216421246528625],\n"," [0.4100499749183655, 0.42182061076164246, 0.4239615499973297],\n"," [0.4173620641231537, 0.41729891300201416, 0.4399716258049011],\n"," [0.3928270936012268, 0.3990015685558319, 0.41375860571861267],\n"," [0.3694362938404083, 0.3781616687774658, 0.39339205622673035],\n"," [0.3839644491672516, 0.41970643401145935, 0.4500277042388916],\n"," [0.3878329396247864, 0.4330573081970215, 0.4415338337421417],\n"," [0.36832481622695923, 0.41305428743362427, 0.4223359525203705],\n"," [0.37021204829216003, 0.41745176911354065, 0.4277096688747406],\n"," [0.37795087695121765, 0.4046395421028137, 0.43042320013046265],\n"," [0.412013977766037, 0.42534640431404114, 0.4294601380825043],\n"," [0.37591055035591125, 0.4000522494316101, 0.429948091506958],\n"," [0.3979351222515106, 0.41125693917274475, 0.41386285424232483],\n"," [0.3823964595794678, 0.41402000188827515, 0.4409475028514862],\n"," [0.3893342614173889, 0.42388632893562317, 0.45426586270332336],\n"," [0.35725000500679016, 0.38564491271972656, 0.41291558742523193],\n"," [0.3706834018230438, 0.4076980948448181, 0.44049590826034546],\n"," [0.39664557576179504, 0.41023001074790955, 0.4128391444683075],\n"," [0.3930477499961853, 0.4151863157749176, 0.43439510464668274],\n"," [0.41837942600250244, 0.41877877712249756, 0.4426179528236389],\n"," [0.4188914895057678, 0.41760480403900146, 0.44032010436058044],\n"," [0.40029096603393555, 0.44513946771621704, 0.4539151191711426],\n"," [0.4207737147808075, 0.42141637206077576, 0.4441033601760864],\n"," [0.4109603464603424, 0.41392284631729126, 0.43788477778434753],\n"," [0.38012251257896423, 0.40439748764038086, 0.42434272170066833],\n"," [0.40358057618141174, 0.4187522530555725, 0.4232218861579895],\n"," [0.3849894106388092, 0.4270126521587372, 0.4375995695590973],\n"," [0.3935147821903229, 0.39575982093811035, 0.42000317573547363],\n"," [0.3878166675567627, 0.4248405694961548, 0.45743823051452637],\n"," [0.389417439699173, 0.42406347393989563, 0.45582151412963867],\n"," [0.41745004057884216, 0.4270673394203186, 0.4292081594467163],\n"," [0.4047168493270874, 0.4181114435195923, 0.42053642868995667],\n"," [0.37760481238365173, 0.41429752111434937, 0.4461856782436371],\n"," [0.3797789514064789, 0.38893887400627136, 0.40476927161216736],\n"," [0.414241760969162, 0.42550379037857056, 0.4304645359516144],\n"," [0.4191116392612457, 0.4341259300708771, 0.4385608434677124],\n"," [0.420284628868103, 0.4197235107421875, 0.4439852237701416],\n"," [0.3872202932834625, 0.41075095534324646, 0.43869245052337646],\n"," [0.3778441250324249, 0.4143960475921631, 0.44716376066207886],\n"," [0.3704553246498108, 0.396366149187088, 0.4272270202636719],\n"," [0.3749515116214752, 0.3848413825035095, 0.40134763717651367],\n"," [0.41893917322158813, 0.4196460545063019, 0.43241000175476074],\n"," [0.4219648540019989, 0.42013269662857056, 0.4329666495323181],\n"," [0.3666161596775055, 0.414655476808548, 0.4250396490097046],\n"," [0.4024510979652405, 0.4032166600227356, 0.4259440302848816],\n"," [0.40405288338661194, 0.4045315086841583, 0.4274718761444092],\n"," [0.41948217153549194, 0.4338940978050232, 0.43876180052757263],\n"," [0.4322809875011444, 0.4444783329963684, 0.44651925563812256],\n"," [0.40160253643989563, 0.4346963167190552, 0.46519631147384644],\n"," [0.3947897255420685, 0.3926183879375458, 0.40713778138160706],\n"," [0.40029922127723694, 0.44152921438217163, 0.4497588276863098],\n"," [0.4139213263988495, 0.41166016459465027, 0.42316433787345886],\n"," [0.39200401306152344, 0.4191955327987671, 0.4474586844444275],\n"," [0.3690233826637268, 0.41319629549980164, 0.42094433307647705],\n"," [0.4225558340549469, 0.42275699973106384, 0.44797641038894653],\n"," [0.36690467596054077, 0.39511480927467346, 0.42213380336761475],\n"," [0.3694027364253998, 0.39783212542533875, 0.42638927698135376],\n"," [0.40778693556785583, 0.40375807881355286, 0.41694021224975586],\n"," [0.38495945930480957, 0.40908312797546387, 0.4386039972305298],\n"," [0.40125927329063416, 0.4088265299797058, 0.42331674695014954],\n"," [0.4249844551086426, 0.4209636449813843, 0.43132489919662476],\n"," [0.3812412619590759, 0.4196474254131317, 0.453663170337677],\n"," [0.41041892766952515, 0.4253547787666321, 0.42844852805137634],\n"," [0.41256916522979736, 0.4249391257762909, 0.42750242352485657],\n"," [0.3721581697463989, 0.4078730642795563, 0.4379638433456421],\n"," [0.4188195765018463, 0.4211508631706238, 0.44795599579811096],\n"," [0.39464351534843445, 0.4203941226005554, 0.4492056965827942],\n"," [0.4045935571193695, 0.4209378957748413, 0.42586520314216614],\n"," [0.41392025351524353, 0.428909033536911, 0.4330083429813385],\n"," [0.3773563504219055, 0.40298014879226685, 0.4304893910884857],\n"," [0.3781881034374237, 0.4157445430755615, 0.4489079713821411],\n"," [0.4071161448955536, 0.40592482686042786, 0.4283980429172516],\n"," [0.4121965765953064, 0.4248538911342621, 0.4274231791496277],\n"," [0.40635642409324646, 0.4213777184486389, 0.4252126216888428],\n"," [0.404700368642807, 0.41752687096595764, 0.41958341002464294],\n"," [0.38476812839508057, 0.41372430324554443, 0.4425064027309418],\n"," [0.3997141718864441, 0.43339040875434875, 0.4630363881587982],\n"," [0.367715448141098, 0.41396021842956543, 0.4237076938152313],\n"," [0.3748965263366699, 0.41899755597114563, 0.4285285472869873],\n"," [0.38999056816101074, 0.3922461271286011, 0.41671350598335266],\n"," [0.38702377676963806, 0.3877979516983032, 0.4001508355140686],\n"," [0.42354559898376465, 0.4379674196243286, 0.44325926899909973],\n"," [0.412868469953537, 0.41296857595443726, 0.4374992549419403],\n"," [0.3947218358516693, 0.41993868350982666, 0.44942986965179443],\n"," [0.4109388589859009, 0.42336344718933105, 0.42576175928115845],\n"," [0.3696005940437317, 0.40752503275871277, 0.44123101234436035],\n"," [0.38942772150039673, 0.4265049993991852, 0.45889773964881897],\n"," [0.4079408347606659, 0.40867364406585693, 0.42242249846458435],\n"," [0.41485244035720825, 0.4273081421852112, 0.4302109181880951],\n"," [0.4115387201309204, 0.4114436209201813, 0.4353145956993103],\n"," [0.4140147864818573, 0.43043261766433716, 0.4349198341369629],\n"," [0.38874977827072144, 0.42431461811065674, 0.45508652925491333],\n"," [0.38854992389678955, 0.4119446277618408, 0.4408937692642212],\n"," [0.3971444368362427, 0.4104937016963959, 0.41291946172714233],\n"," [0.3562898337841034, 0.402681827545166, 0.4127311408519745],\n"," [0.4164840579032898, 0.430304616689682, 0.43344220519065857],\n"," [0.41443967819213867, 0.42691540718078613, 0.4290175437927246],\n"," [0.3943576514720917, 0.42005589604377747, 0.4467896521091461],\n"," [0.4120691120624542, 0.4118119776248932, 0.42433053255081177],\n"," [0.4136788547039032, 0.42652690410614014, 0.4287513494491577],\n"," [0.4229937195777893, 0.4228488802909851, 0.43584349751472473],\n"," [0.3757491707801819, 0.4126857817173004, 0.4454204738140106],\n"," [0.3921179473400116, 0.39906740188598633, 0.41361087560653687],\n"," [0.3891769051551819, 0.43350890278816223, 0.4415976107120514],\n"," [0.3945561945438385, 0.4190548360347748, 0.44546326994895935],\n"," [0.3691476583480835, 0.415481835603714, 0.4251760244369507],\n"," [0.37730446457862854, 0.39919209480285645, 0.41644352674484253],\n"," [0.4092634618282318, 0.4223530888557434, 0.4243481159210205],\n"," [0.3917292654514313, 0.42423367500305176, 0.4526946544647217],\n"," [0.39290645718574524, 0.4280531108379364, 0.4589826762676239],\n"," [0.39879241585731506, 0.41937384009361267, 0.45070329308509827],\n"," [0.39828944206237793, 0.39887604117393494, 0.4216102957725525],\n"," [0.37101808190345764, 0.4090362787246704, 0.44219860434532166],\n"," [0.3706500828266144, 0.4066105782985687, 0.43704771995544434],\n"," [0.3691597878932953, 0.4155716300010681, 0.4253076910972595],\n"," [0.4268297553062439, 0.4383561313152313, 0.44125717878341675],\n"," [0.38230594992637634, 0.4182482659816742, 0.44866126775741577],\n"," [0.37687432765960693, 0.41359448432922363, 0.4441497325897217],\n"," [0.382952481508255, 0.4064593017101288, 0.4311582148075104],\n"," [0.40844106674194336, 0.42333731055259705, 0.42635470628738403],\n"," [0.38815051317214966, 0.4330112636089325, 0.4422599673271179],\n"," [0.4182465672492981, 0.4164297878742218, 0.4285297691822052],\n"," [0.4108133018016815, 0.4234696924686432, 0.42620277404785156],\n"," [0.41321852803230286, 0.4264262914657593, 0.4303738474845886],\n"," [0.4127638638019562, 0.4173128306865692, 0.430658221244812],\n"," [0.4029830992221832, 0.4377308189868927, 0.4693566560745239],\n"," [0.41587477922439575, 0.4294110834598541, 0.43269068002700806],\n"," [0.3933783173561096, 0.4383252263069153, 0.4475073516368866],\n"," [0.36563706398010254, 0.409467488527298, 0.4169551432132721],\n"," [0.41165775060653687, 0.41382715106010437, 0.44053688645362854],\n"," [0.3836183249950409, 0.426699161529541, 0.43486979603767395],\n"," [0.4079136848449707, 0.40666183829307556, 0.41809549927711487],\n"," [0.3705274164676666, 0.408311128616333, 0.4412328004837036],\n"," [0.40768471360206604, 0.4069329500198364, 0.41814208030700684],\n"," [0.394225150346756, 0.408943772315979, 0.412095308303833],\n"," [0.37095120549201965, 0.40790027379989624, 0.4396209418773651],\n"," [0.39252519607543945, 0.39044877886772156, 0.4046366512775421],\n"," [0.40662214159965515, 0.42188870906829834, 0.4272731840610504],\n"," [0.41173872351646423, 0.4098058342933655, 0.42119842767715454],\n"," [0.39188113808631897, 0.4279322028160095, 0.45975491404533386],\n"," [0.38656535744667053, 0.4101414084434509, 0.428850382566452],\n"," [0.4008043110370636, 0.4165153205394745, 0.42156293988227844],\n"," [0.3833134174346924, 0.39264434576034546, 0.4086838662624359],\n"," [0.4215601682662964, 0.4388916492462158, 0.4421268403530121],\n"," [0.4231465458869934, 0.4226456582546234, 0.4472745358943939],\n"," [0.39001426100730896, 0.4161671996116638, 0.4441247284412384],\n"," [0.3837904930114746, 0.41824254393577576, 0.44915521144866943],\n"," [0.3826162815093994, 0.41888734698295593, 0.45020830631256104],\n"," [0.38438886404037476, 0.41653820872306824, 0.4446999430656433],\n"," [0.4111054539680481, 0.43040335178375244, 0.43505316972732544],\n"," [0.37850555777549744, 0.4033358693122864, 0.4294288754463196],\n"," [0.4082178473472595, 0.4074346721172333, 0.41958341002464294],\n"," [0.4014454185962677, 0.4169676601886749, 0.42144328355789185],\n"," [0.3979213833808899, 0.4302871823310852, 0.46164703369140625],\n"," [0.41172003746032715, 0.41340208053588867, 0.43929293751716614],\n"," [0.4091048240661621, 0.42383337020874023, 0.4283987879753113],\n"," [0.40705034136772156, 0.4071134030818939, 0.4299848973751068],\n"," [0.42122623324394226, 0.4334130585193634, 0.4356435239315033],\n"," [0.4149571657180786, 0.4297772943973541, 0.4333708882331848],\n"," [0.41058409214019775, 0.4109814763069153, 0.4235188663005829],\n"," [0.37641653418540955, 0.42307838797569275, 0.4322742223739624],\n"," [0.4032541513442993, 0.425112783908844, 0.4532742500305176],\n"," [0.39711159467697144, 0.4204762876033783, 0.4443742334842682],\n"," [0.3867858946323395, 0.4235656261444092, 0.4551311433315277],\n"," [0.3921610116958618, 0.4081175923347473, 0.41252997517585754],\n"," [0.4046373665332794, 0.43654975295066833, 0.44027113914489746],\n"," [0.38486695289611816, 0.4093686044216156, 0.43958815932273865],\n"," [0.40470150113105774, 0.41792428493499756, 0.4204283654689789],\n"," [0.40748491883277893, 0.4193967580795288, 0.4297035038471222],\n"," [0.3738200068473816, 0.40900418162345886, 0.43978017568588257],\n"," [0.38772645592689514, 0.4249117970466614, 0.4575619101524353],\n"," [0.41268476843833923, 0.427874892950058, 0.429965078830719],\n"," [0.3768094480037689, 0.4019996225833893, 0.4325244426727295],\n"," [0.4065902531147003, 0.42237868905067444, 0.42680665850639343],\n"," [0.39874520897865295, 0.40041583776474, 0.4249700903892517],\n"," [0.3609880208969116, 0.3878672420978546, 0.41853734850883484],\n"," [0.39691197872161865, 0.42022794485092163, 0.43881165981292725],\n"," [0.4106273353099823, 0.4236415922641754, 0.4268743097782135],\n"," [0.41317418217658997, 0.4126501977443695, 0.4349897801876068],\n"," [0.41686391830444336, 0.41914916038513184, 0.4288369417190552],\n"," [0.39855024218559265, 0.4338591396808624, 0.4655930697917938],\n"," [0.40188372135162354, 0.40433913469314575, 0.4309273958206177],\n"," [0.39593082666397095, 0.4264920949935913, 0.4572398066520691],\n"," [0.389904260635376, 0.43605345487594604, 0.4461706876754761],\n"," [0.4121256470680237, 0.42400631308555603, 0.4284404516220093],\n"," [0.4011619985103607, 0.41530293226242065, 0.41878214478492737],\n"," [0.4214196801185608, 0.43535032868385315, 0.4397425055503845],\n"," [0.41653671860694885, 0.4156511127948761, 0.4273906648159027],\n"," [0.4132358133792877, 0.42517581582069397, 0.4295291602611542],\n"," [0.39204278588294983, 0.4177260994911194, 0.44559529423713684],\n"," [0.390398770570755, 0.42564815282821655, 0.45699313282966614],\n"," [0.4122241139411926, 0.4099629819393158, 0.42164215445518494],\n"," [0.4021311104297638, 0.43823331594467163, 0.47095972299575806],\n"," [0.4092678129673004, 0.4075487554073334, 0.41868722438812256],\n"," [0.3779904842376709, 0.4153348207473755, 0.4482566714286804],\n"," [0.3895999789237976, 0.42612892389297485, 0.4580093026161194],\n"," [0.36695021390914917, 0.41151684522628784, 0.41962775588035583],\n"," [0.37126657366752625, 0.40876108407974243, 0.43955421447753906],\n"," [0.3792317509651184, 0.4114263951778412, 0.43875667452812195],\n"," [0.4025137722492218, 0.4032435417175293, 0.4279586970806122],\n"," [0.3749676048755646, 0.4113107919692993, 0.4421645998954773],\n"," [0.41816118359565735, 0.41782650351524353, 0.4302785396575928],\n"," [0.3831224739551544, 0.40660619735717773, 0.4257701635360718],\n"," [0.39415866136550903, 0.4202750027179718, 0.4480569362640381],\n"," [0.37542203068733215, 0.41156676411628723, 0.44513776898384094],\n"," [0.38819199800491333, 0.42371949553489685, 0.4543144702911377],\n"," [0.4161456227302551, 0.4170112609863281, 0.44215697050094604],\n"," [0.3711545467376709, 0.4004631042480469, 0.4281926155090332],\n"," [0.398428738117218, 0.4221663475036621, 0.4510386288166046],\n"," [0.3929465115070343, 0.40108945965766907, 0.41593772172927856],\n"," [0.37778550386428833, 0.4142876863479614, 0.44804638624191284],\n"," [0.39226898550987244, 0.4057037830352783, 0.40778273344039917],\n"," [0.39057067036628723, 0.4046492278575897, 0.40763458609580994],\n"," [0.3873683512210846, 0.3956991732120514, 0.410317987203598],\n"," [0.39857763051986694, 0.40609049797058105, 0.420684814453125],\n"," [0.4229922890663147, 0.4342501163482666, 0.43523964285850525],\n"," [0.38413724303245544, 0.4182684123516083, 0.44892337918281555],\n"," [0.4221266210079193, 0.4234049320220947, 0.436718612909317],\n"," [0.38110578060150146, 0.4166329503059387, 0.4502122104167938],\n"," [0.41907232999801636, 0.43041226267814636, 0.4320729672908783],\n"," [0.39535561203956604, 0.43054160475730896, 0.46216467022895813],\n"," [0.3905206024646759, 0.42654308676719666, 0.4569553732872009],\n"," [0.41283848881721497, 0.42498064041137695, 0.4270094633102417],\n"," [0.3943307101726532, 0.39691048860549927, 0.4102901816368103],\n"," [0.3736798167228699, 0.41902366280555725, 0.4284554421901703],\n"," [0.3686911463737488, 0.3950533866882324, 0.42562994360923767],\n"," [0.37825754284858704, 0.3879814147949219, 0.4043407142162323],\n"," [0.3779205083847046, 0.4146243929862976, 0.4453602731227875],\n"," [0.3848250210285187, 0.42744967341423035, 0.43422752618789673],\n"," [0.38142380118370056, 0.4097434878349304, 0.43884924054145813],\n"," [0.3824084401130676, 0.40515196323394775, 0.4230673015117645],\n"," [0.42542245984077454, 0.4288303852081299, 0.45164182782173157],\n"," [0.4165583550930023, 0.41678813099861145, 0.4387926757335663],\n"," [0.39134472608566284, 0.4135456085205078, 0.4315700829029083],\n"," [0.39534953236579895, 0.39740508794784546, 0.41065889596939087],\n"," [0.3934587240219116, 0.42883315682411194, 0.4601288139820099],\n"," [0.396716833114624, 0.4045400023460388, 0.4193516671657562],\n"," [0.4052450358867645, 0.41932666301727295, 0.42356279492378235],\n"," [0.37270045280456543, 0.39540940523147583, 0.41315335035324097],\n"," [0.4006708562374115, 0.40213724970817566, 0.4261225759983063],\n"," [0.4088137149810791, 0.42246168851852417, 0.4249541759490967],\n"," [0.4109101891517639, 0.4135062098503113, 0.42737430334091187],\n"," [0.3860805630683899, 0.4215976595878601, 0.4529117941856384],\n"," [0.42223674058914185, 0.4227149784564972, 0.44719454646110535],\n"," [0.4164256155490875, 0.43218737840652466, 0.43721631169319153],\n"," [0.41890907287597656, 0.41869571805000305, 0.44355276226997375],\n"," [0.3781886100769043, 0.4019596576690674, 0.42122164368629456],\n"," [0.413935124874115, 0.4271322786808014, 0.4301941990852356],\n"," [0.410983681678772, 0.41229233145713806, 0.4379262328147888],\n"," [0.3779944181442261, 0.4042190909385681, 0.4313852787017822],\n"," [0.38918444514274597, 0.4312267303466797, 0.43907734751701355],\n"," [0.3784152865409851, 0.4012179374694824, 0.4195939004421234],\n"," [0.41220247745513916, 0.42667973041534424, 0.43104055523872375],\n"," [0.4134330749511719, 0.41576242446899414, 0.44243356585502625],\n"," [0.41164731979370117, 0.42678332328796387, 0.4304712116718292],\n"," [0.3742530941963196, 0.38301509618759155, 0.39768826961517334],\n"," [0.4254820644855499, 0.42534393072128296, 0.44999372959136963],\n"," [0.4103701412677765, 0.4254077076911926, 0.4305114448070526],\n"," [0.3974343240261078, 0.4183916449546814, 0.44503259658813477],\n"," [0.39751461148262024, 0.4335891902446747, 0.466156542301178],\n"," [0.3813263773918152, 0.40910759568214417, 0.43660739064216614],\n"," [0.36964553594589233, 0.39913854002952576, 0.42750024795532227],\n"," [0.3940872251987457, 0.4165525734424591, 0.44386813044548035],\n"," [0.4070521295070648, 0.42117294669151306, 0.4239700734615326],\n"," [0.39003273844718933, 0.4353710412979126, 0.4453989863395691],\n"," [0.38555195927619934, 0.42720159888267517, 0.4559929370880127],\n"," [0.40988945960998535, 0.4112010896205902, 0.43695998191833496],\n"," [0.4207661747932434, 0.42052310705184937, 0.4440746009349823],\n"," [0.3790612518787384, 0.4057801365852356, 0.4358691871166229],\n"," [0.3891628086566925, 0.41840869188308716, 0.4454363286495209],\n"," [0.4179157316684723, 0.4176751375198364, 0.44235101342201233],\n"," [0.39719122648239136, 0.40863609313964844, 0.40936511754989624],\n"," [0.4099162817001343, 0.425650417804718, 0.4312034249305725],\n"," [0.38808363676071167, 0.41297706961631775, 0.43906170129776],\n"," [0.41517215967178345, 0.4469897747039795, 0.45113807916641235],\n"," [0.3852086067199707, 0.43008267879486084, 0.43858951330184937],\n"," [0.40032193064689636, 0.42545226216316223, 0.4559665620326996],\n"," [0.4047703742980957, 0.4045747220516205, 0.416565477848053],\n"," [0.4227602779865265, 0.43693670630455017, 0.441094309091568],\n"," [0.4061024785041809, 0.40459659695625305, 0.4170721769332886],\n"," [0.3818305730819702, 0.41641467809677124, 0.4455724060535431],\n"," [0.4189068675041199, 0.43092259764671326, 0.4324858784675598],\n"," [0.3880765438079834, 0.39581945538520813, 0.4101293087005615],\n"," [0.40628597140312195, 0.4207741916179657, 0.42486175894737244],\n"," [0.3730904757976532, 0.416741281747818, 0.42523935437202454],\n"," [0.41656070947647095, 0.4156988561153412, 0.4277748465538025],\n"," [0.38046789169311523, 0.4171159565448761, 0.44781482219696045],\n"," [0.39327821135520935, 0.409818172454834, 0.41348645091056824],\n"," [0.4002572298049927, 0.4331168830394745, 0.46484628319740295],\n"," [0.39002951979637146, 0.42600148916244507, 0.4583207368850708],\n"," [0.3780653774738312, 0.42387479543685913, 0.43368807435035706],\n"," [0.4082726538181305, 0.4207635223865509, 0.4248825013637543],\n"," [0.3930940330028534, 0.4093453884124756, 0.41300663352012634],\n"," [0.4148440361022949, 0.4116695821285248, 0.4254312813282013],\n"," [0.4014061987400055, 0.40188559889793396, 0.41490480303764343],\n"," [0.3835527002811432, 0.4107194244861603, 0.4387279450893402],\n"," [0.38784661889076233, 0.4236830174922943, 0.4551723003387451],\n"," [0.3935631215572357, 0.4032851755619049, 0.42268040776252747],\n"," [0.38441166281700134, 0.4193388521671295, 0.45098432898521423],\n"," [0.42047831416130066, 0.4199829399585724, 0.4428400993347168],\n"," [0.41809338331222534, 0.4168204069137573, 0.42755740880966187],\n"," [0.39583849906921387, 0.3962332010269165, 0.40826350450515747],\n"," [0.4040514826774597, 0.4174354374408722, 0.42039754986763],\n"," [0.3733961284160614, 0.41002756357192993, 0.4425492584705353],\n"," [0.38617807626724243, 0.4225000739097595, 0.45473814010620117],\n"," [0.3775242269039154, 0.4051850736141205, 0.43331798911094666],\n"," [0.4118748605251312, 0.4269096255302429, 0.4328949451446533],\n"," [0.419683039188385, 0.415120929479599, 0.42795541882514954],\n"," [0.3864780366420746, 0.4129973351955414, 0.44058409333229065],\n"," [0.4143412113189697, 0.4131225347518921, 0.425005704164505],\n"," [0.4056108891963959, 0.40648049116134644, 0.4311104714870453],\n"," [0.3820095360279083, 0.41779661178588867, 0.4496661126613617],\n"," [0.37933629751205444, 0.41538408398628235, 0.4488300681114197],\n"," [0.3937690258026123, 0.4219266176223755, 0.44987359642982483],\n"," [0.40906858444213867, 0.4225864112377167, 0.4250762462615967],\n"," [0.40713754296302795, 0.4183838367462158, 0.41926756501197815],\n"," [0.38831278681755066, 0.3963273763656616, 0.4115466773509979],\n"," [0.40731558203697205, 0.42241016030311584, 0.42691412568092346],\n"," [0.3908035457134247, 0.42563706636428833, 0.45744502544403076],\n"," [0.3719065189361572, 0.40050676465034485, 0.4293598234653473],\n"," [0.37834540009498596, 0.4197411835193634, 0.42887917160987854],\n"," [0.42019525170326233, 0.4205351769924164, 0.4452190697193146],\n"," [0.37424489855766296, 0.4176828861236572, 0.42543932795524597],\n"," [0.4267888069152832, 0.44079312682151794, 0.4478186368942261],\n"," [0.4337875247001648, 0.4360334575176239, 0.4559384882450104],\n"," [0.39848920702934265, 0.3992269039154053, 0.41191408038139343],\n"," [0.4166528284549713, 0.4158068001270294, 0.4285915791988373],\n"," [0.4105739891529083, 0.4113456606864929, 0.4374207556247711],\n"," [0.4062037467956543, 0.4178515374660492, 0.4190930724143982],\n"," [0.3734302818775177, 0.4101506173610687, 0.44259583950042725],\n"," [0.37939098477363586, 0.41481053829193115, 0.44523629546165466],\n"," [0.40455135703086853, 0.4169078767299652, 0.4193192422389984],\n"," [0.387846976518631, 0.4134756922721863, 0.4417146146297455],\n"," [0.41805800795555115, 0.431780070066452, 0.436215877532959],\n"," [0.4131156802177429, 0.4259847402572632, 0.4297383427619934],\n"," [0.377231240272522, 0.40330222249031067, 0.4300270974636078],\n"," [0.4092961549758911, 0.42182061076164246, 0.423345148563385],\n"," [0.387511670589447, 0.4292311668395996, 0.435018926858902],\n"," [0.39182615280151367, 0.4167041778564453, 0.4431281089782715],\n"," [0.3815982937812805, 0.4064663052558899, 0.4322013854980469],\n"," [0.39087027311325073, 0.42798352241516113, 0.4605546295642853],\n"," [0.4303305447101593, 0.429573655128479, 0.4543011784553528],\n"," [0.41292494535446167, 0.4124920964241028, 0.4354779124259949],\n"," [0.38810068368911743, 0.42248010635375977, 0.45269474387168884],\n"," [0.3679478168487549, 0.39305806159973145, 0.41315484046936035],\n"," [0.3946651816368103, 0.4231155216693878, 0.45173025131225586],\n"," [0.3953837454319, 0.3989596366882324, 0.42572659254074097],\n"," [0.3992156982421875, 0.4335794150829315, 0.4421604573726654],\n"," [0.369574636220932, 0.4159846901893616, 0.4257286489009857],\n"," [0.4262952208518982, 0.4255702495574951, 0.4503082036972046],\n"," [0.3867171108722687, 0.421447217464447, 0.45140448212623596],\n"," [0.390524297952652, 0.4246164560317993, 0.4545920193195343],\n"," [0.358786404132843, 0.40259629487991333, 0.4099242091178894],\n"," [0.3847310543060303, 0.4209046959877014, 0.453227698802948],\n"," [0.4159492254257202, 0.4296203851699829, 0.4328835606575012],\n"," [0.40018734335899353, 0.4244430661201477, 0.4509732723236084],\n"," [0.41168656945228577, 0.4112319052219391, 0.434104323387146],\n"," [0.35937032103538513, 0.3861766755580902, 0.41754433512687683],\n"," [0.3961063325405121, 0.41320523619651794, 0.41787973046302795],\n"," [0.4078232944011688, 0.41936033964157104, 0.4233337342739105],\n"," [0.39750024676322937, 0.42291778326034546, 0.45046085119247437],\n"," [0.417293518781662, 0.41657018661499023, 0.43849554657936096],\n"," [0.38543710112571716, 0.40991511940956116, 0.43913501501083374],\n"," [0.3650786876678467, 0.37526392936706543, 0.39110347628593445],\n"," [0.40383172035217285, 0.4156440496444702, 0.41739222407341003],\n"," [0.38422125577926636, 0.42123961448669434, 0.4404037296772003],\n"," [0.40631726384162903, 0.40726321935653687, 0.4319397211074829],\n"," [0.3895145654678345, 0.4143776297569275, 0.44350704550743103],\n"," [0.3791118860244751, 0.40404844284057617, 0.43395403027534485],\n"," [0.3879830241203308, 0.4132963716983795, 0.44153809547424316],\n"," [0.4180203378200531, 0.4173215627670288, 0.4391680359840393],\n"," [0.37183231115341187, 0.4165247678756714, 0.4255879819393158],\n"," [0.38407373428344727, 0.41069790720939636, 0.438874214887619],\n"," [0.4070776402950287, 0.4187614917755127, 0.42076730728149414],\n"," [0.40868252515792847, 0.42289823293685913, 0.4263167977333069],\n"," [0.3959594964981079, 0.4185619652271271, 0.442739874124527],\n"," [0.3904479742050171, 0.4243927299976349, 0.4542723000049591],\n"," [0.36469462513923645, 0.3911343514919281, 0.41708904504776],\n"," [0.3780471682548523, 0.42167189717292786, 0.428908109664917],\n"," [0.3618628978729248, 0.3886852562427521, 0.41928115487098694],\n"," [0.401921808719635, 0.4037051200866699, 0.41702234745025635],\n"," [0.4225756525993347, 0.42241641879081726, 0.44670751690864563],\n"," [0.3975618779659271, 0.431334525346756, 0.46329373121261597],\n"," [0.4257877469062805, 0.4292353689670563, 0.4515899419784546],\n"," [0.40677231550216675, 0.4068933129310608, 0.43062785267829895],\n"," [0.3819521963596344, 0.4254342317581177, 0.4327390491962433],\n"," [0.3944586515426636, 0.42313653230667114, 0.45154687762260437],\n"," [0.4296296238899231, 0.4433351159095764, 0.447579562664032],\n"," [0.4027806222438812, 0.41786614060401917, 0.42215704917907715],\n"," [0.3782446086406708, 0.4127620756626129, 0.44171229004859924],\n"," [0.39581960439682007, 0.41790762543678284, 0.44458505511283875],\n"," [0.41542890667915344, 0.42785710096359253, 0.4308362305164337],\n"," [0.3882983326911926, 0.43331119418144226, 0.44259729981422424],\n"," [0.4116390347480774, 0.4123500883579254, 0.437409371137619],\n"," [0.4090084135532379, 0.4111284911632538, 0.43731456995010376],\n"," [0.41785815358161926, 0.43042922019958496, 0.4327095150947571],\n"," [0.39171838760375977, 0.42744386196136475, 0.45879030227661133],\n"," [0.37580621242523193, 0.40499410033226013, 0.4334241449832916],\n"," [0.41129836440086365, 0.4242153465747833, 0.42733150720596313],\n"," [0.4231666326522827, 0.4205623269081116, 0.4321942925453186],\n"," [0.425628662109375, 0.4253446161746979, 0.4485108256340027],\n"," [0.42186495661735535, 0.43303048610687256, 0.43442627787590027],\n"," [0.394919216632843, 0.42766091227531433, 0.4595467746257782],\n"," [0.41611266136169434, 0.4167911112308502, 0.4406377971172333],\n"," [0.39665791392326355, 0.4367097020149231, 0.44733965396881104],\n"," [0.419164776802063, 0.41941913962364197, 0.44452372193336487],\n"," [0.40937983989715576, 0.42222052812576294, 0.4247009754180908],\n"," [0.4074695110321045, 0.42339977622032166, 0.427234023809433],\n"," [0.3795692026615143, 0.407117635011673, 0.4346822500228882],\n"," [0.4191725254058838, 0.4324185848236084, 0.4360560178756714],\n"," [0.42139706015586853, 0.41965293884277344, 0.43145307898521423],\n"," [0.3635900914669037, 0.4103245735168457, 0.4200303256511688],\n"," [0.4277119040489197, 0.43973541259765625, 0.44210895895957947],\n"," [0.4025629758834839, 0.4186898469924927, 0.42415672540664673],\n"," [0.3706798553466797, 0.4073566198348999, 0.43855908513069153],\n"," [0.3928990662097931, 0.40607157349586487, 0.4112950563430786],\n"," [0.37778475880622864, 0.4217394292354584, 0.42982837557792664],\n"," [0.38185104727745056, 0.41766157746315, 0.4487378001213074],\n"," [0.4068203270435333, 0.4185120463371277, 0.4211573600769043],\n"," [0.3953022360801697, 0.42285627126693726, 0.45097455382347107],\n"," [0.3824883997440338, 0.4081175625324249, 0.43541640043258667],\n"," [0.39190050959587097, 0.416545569896698, 0.4455397129058838],\n"," [0.4002723693847656, 0.41476970911026, 0.4177325367927551],\n"," [0.36774224042892456, 0.3958528935909271, 0.4230550229549408],\n"," [0.38628366589546204, 0.4309588074684143, 0.4398578405380249],\n"," [0.4120665192604065, 0.4115563631057739, 0.4237821102142334],\n"," [0.3849133551120758, 0.4095999300479889, 0.4399402439594269],\n"," [0.37676408886909485, 0.40290793776512146, 0.4299989938735962],\n"," [0.3612825870513916, 0.3889395594596863, 0.4162265658378601],\n"," [0.40746793150901794, 0.41961246728897095, 0.4215863347053528],\n"," [0.41321244835853577, 0.4259726107120514, 0.42927563190460205],\n"," [0.3758864104747772, 0.4217790365219116, 0.4315626919269562],\n"," [0.36366045475006104, 0.4096767008304596, 0.4182831048965454],\n"," [0.3996264934539795, 0.41216644644737244, 0.4139232933521271],\n"," [0.3977055847644806, 0.41458749771118164, 0.41899746656417847],\n"," [0.4229949116706848, 0.42319637537002563, 0.44847384095191956],\n"," [0.4110660254955292, 0.4252374470233917, 0.42794114351272583],\n"," [0.38163796067237854, 0.3896021544933319, 0.4038732051849365],\n"," [0.40523403882980347, 0.4186910092830658, 0.4215784966945648],\n"," [0.42789319157600403, 0.44266119599342346, 0.4495849907398224],\n"," [0.4140453636646271, 0.42721617221832275, 0.43003809452056885],\n"," [0.4198139011859894, 0.4193580746650696, 0.44431406259536743],\n"," [0.3654787242412567, 0.40285563468933105, 0.435653954744339],\n"," [0.3930104970932007, 0.42644572257995605, 0.4548519551753998],\n"," [0.36924004554748535, 0.39274880290031433, 0.41167500615119934],\n"," [0.3871719241142273, 0.41203537583351135, 0.4401415288448334],\n"," [0.3882051110267639, 0.4161519408226013, 0.4443870782852173],\n"," [0.37554699182510376, 0.4123552739620209, 0.44309988617897034],\n"," [0.39453908801078796, 0.4174044132232666, 0.43610307574272156],\n"," [0.42530882358551025, 0.4251374304294586, 0.44908514618873596],\n"," [0.3729363679885864, 0.3993145823478699, 0.42619451880455017],\n"," [0.38521596789360046, 0.41240617632865906, 0.4403514564037323],\n"," [0.4118879735469818, 0.42588454484939575, 0.4285000264644623],\n"," [0.4235394597053528, 0.43609118461608887, 0.43794751167297363],\n"," [0.40383976697921753, 0.4167483448982239, 0.41948139667510986],\n"," [0.4077615439891815, 0.42059165239334106, 0.42255645990371704],\n"," [0.4250718653202057, 0.44058555364608765, 0.4468519687652588],\n"," [0.4075947701931, 0.40864166617393494, 0.433373361825943],\n"," [0.41317617893218994, 0.41216981410980225, 0.423388808965683],\n"," [0.3930386006832123, 0.4071687161922455, 0.4104350805282593],\n"," [0.3836851418018341, 0.4088923931121826, 0.4356014132499695],\n"," [0.38229870796203613, 0.4196864068508148, 0.45323649048805237],\n"," [0.41359540820121765, 0.4156867563724518, 0.4393608570098877],\n"," [0.41574031114578247, 0.41385704278945923, 0.42513617873191833],\n"," [0.4111201763153076, 0.42316895723342896, 0.42449989914894104],\n"," [0.4131790101528168, 0.41578954458236694, 0.4424554109573364],\n"," [0.3822486400604248, 0.4055100381374359, 0.4336283504962921],\n"," [0.40861883759498596, 0.4094045162200928, 0.43319323658943176],\n"," [0.3910089135169983, 0.4198644161224365, 0.4487595856189728],\n"," [0.3734720051288605, 0.4087114930152893, 0.43990176916122437],\n"," [0.3963416814804077, 0.4208470284938812, 0.4476754665374756],\n"," [0.4060056209564209, 0.42028236389160156, 0.42465999722480774],\n"," [0.3861820697784424, 0.42096593976020813, 0.45070913434028625],\n"," [0.3614742159843445, 0.407061904668808, 0.41581815481185913],\n"," [0.4145068824291229, 0.41645699739456177, 0.42851972579956055],\n"," [0.4001905620098114, 0.4455357491970062, 0.45608940720558167],\n"," [0.4367973804473877, 0.4488626718521118, 0.4521077573299408],\n"," [0.41753602027893066, 0.41805368661880493, 0.4426840543746948],\n"," [0.36274784803390503, 0.39071527123451233, 0.41736578941345215],\n"," [0.39778783917427063, 0.4290962219238281, 0.4561741352081299],\n"," [0.36140602827072144, 0.3867625296115875, 0.4175300598144531],\n"," [0.39853715896606445, 0.4231596887111664, 0.44953715801239014],\n"," [0.3915572762489319, 0.40434765815734863, 0.40646541118621826],\n"," [0.3736732602119446, 0.4007839560508728, 0.42785075306892395],\n"," [0.39166784286499023, 0.4286603033542633, 0.46192917227745056],\n"," [0.4106970429420471, 0.42333489656448364, 0.4262029826641083],\n"," [0.3943023681640625, 0.42803266644477844, 0.4561007618904114],\n"," [0.42377081513404846, 0.4361703097820282, 0.4386941194534302],\n"," [0.3937322199344635, 0.42796996235847473, 0.4619322419166565],\n"," [0.4004385471343994, 0.43626248836517334, 0.4688233435153961],\n"," [0.3962751626968384, 0.4194229245185852, 0.44207513332366943],\n"," [0.3853203058242798, 0.4215067923069, 0.45380762219429016],\n"," [0.37372705340385437, 0.4094713032245636, 0.442623108625412],\n"," [0.42747730016708374, 0.4240863621234894, 0.43561413884162903],\n"," [0.39510461688041687, 0.41624683141708374, 0.43377378582954407],\n"," [0.4007682502269745, 0.4186411201953888, 0.4457470178604126],\n"," [0.4074731171131134, 0.41896307468414307, 0.42102864384651184],\n"," [0.41568416357040405, 0.4304959177970886, 0.4341230094432831],\n"," [0.3855879604816437, 0.4204290807247162, 0.449921578168869],\n"," [0.40087759494781494, 0.43781983852386475, 0.44562438130378723],\n"," [0.42264440655708313, 0.4361456334590912, 0.4397180378437042],\n"," [0.3671838343143463, 0.39289993047714233, 0.42166072130203247],\n"," [0.3609962463378906, 0.3970937132835388, 0.42914170026779175],\n"," [0.37645068764686584, 0.4133153259754181, 0.44641074538230896],\n"," [0.4073095917701721, 0.4091870188713074, 0.4360654354095459],\n"," [0.40271496772766113, 0.418181449174881, 0.42333000898361206],\n"," [0.37262213230133057, 0.39906397461891174, 0.4262329936027527],\n"," [0.3801558017730713, 0.41877132654190063, 0.4530925452709198],\n"," [0.4142623543739319, 0.4137006402015686, 0.42648813128471375],\n"," [0.3635222017765045, 0.4070068597793579, 0.4144262969493866],\n"," [0.4153698980808258, 0.4147467017173767, 0.4374970495700836],\n"," [0.4188670217990875, 0.42156997323036194, 0.4489777982234955],\n"," [0.37585726380348206, 0.4131948947906494, 0.44558295607566833],\n"," [0.39320388436317444, 0.4101172983646393, 0.4152964651584625],\n"," [0.3705568015575409, 0.40800192952156067, 0.4411194920539856],\n"," [0.4172600209712982, 0.41629457473754883, 0.43865862488746643],\n"," [0.3640672266483307, 0.4101272225379944, 0.41966670751571655],\n"," [0.37725311517715454, 0.4130968451499939, 0.4451686441898346],\n"," [0.41075679659843445, 0.42274948954582214, 0.42771539092063904],\n"," [0.4149026572704315, 0.41350436210632324, 0.4248812198638916],\n"," [0.41420236229896545, 0.425650417804718, 0.42783164978027344],\n"," [0.4075961112976074, 0.41210323572158813, 0.4251057803630829],\n"," [0.38006502389907837, 0.40511342883110046, 0.43583032488822937],\n"," [0.41165685653686523, 0.411028653383255, 0.43370822072029114],\n"," [0.38717955350875854, 0.42301633954048157, 0.4545353353023529],\n"," [0.3734425902366638, 0.4091404974460602, 0.43989038467407227],\n"," [0.3867771029472351, 0.42437413334846497, 0.456676185131073],\n"," [0.418216735124588, 0.41903918981552124, 0.4437980651855469],\n"," [0.4055018424987793, 0.4053577482700348, 0.42785441875457764],\n"," [0.41773414611816406, 0.43282926082611084, 0.43805721402168274],\n"," [0.3827873468399048, 0.41673874855041504, 0.44837725162506104],\n"," [0.4238032102584839, 0.4276752769947052, 0.4393332600593567],\n"," [0.4119240939617157, 0.42311644554138184, 0.42503538727760315],\n"," [0.41791653633117676, 0.43000441789627075, 0.4319395422935486],\n"," [0.38948023319244385, 0.42506474256515503, 0.456874817609787],\n"," [0.4054928719997406, 0.4185275137424469, 0.42239201068878174],\n"," [0.3852304220199585, 0.4287005364894867, 0.43656259775161743],\n"," [0.37641119956970215, 0.41343194246292114, 0.4220450222492218],\n"," [0.39646580815315247, 0.43168407678604126, 0.4634688198566437],\n"," [0.3888522684574127, 0.41089731454849243, 0.4288538992404938],\n"," [0.40361684560775757, 0.4038795828819275, 0.41667476296424866],\n"," [0.38137003779411316, 0.4128839075565338, 0.44512128829956055],\n"," [0.41770538687705994, 0.4151591360569, 0.426663339138031],\n"," [0.4062988758087158, 0.4227316677570343, 0.4281175136566162],\n"," [0.421957790851593, 0.43651652336120605, 0.44125697016716003],\n"," [0.41986462473869324, 0.43711116909980774, 0.44540879130363464],\n"," [0.42312872409820557, 0.4317244291305542, 0.43308523297309875],\n"," [0.408917635679245, 0.40840932726860046, 0.43028897047042847],\n"," [0.41603711247444153, 0.4188467264175415, 0.4453746974468231],\n"," [0.39622342586517334, 0.40371841192245483, 0.4182584583759308],\n"," [0.3966502249240875, 0.44199246168136597, 0.45188748836517334],\n"," [0.39357706904411316, 0.4399523138999939, 0.45006701350212097],\n"," [0.36071282625198364, 0.4076712429523468, 0.41836774349212646],\n"," [0.39021438360214233, 0.399018257856369, 0.41434580087661743],\n"," [0.40901613235473633, 0.42386627197265625, 0.42850548028945923],\n"," [0.39310771226882935, 0.41623276472091675, 0.435315877199173],\n"," [0.4210296869277954, 0.41892358660697937, 0.43173328042030334],\n"," [0.37896791100502014, 0.4133692681789398, 0.4436667561531067],\n"," [0.3691605031490326, 0.4154091775417328, 0.4247971475124359],\n"," [0.3714313209056854, 0.4083835184574127, 0.440255343914032],\n"," [0.41114309430122375, 0.4098224937915802, 0.42101868987083435],\n"," [0.39206913113594055, 0.41794484853744507, 0.44562262296676636],\n"," [0.4110337495803833, 0.4104907810688019, 0.4341498017311096],\n"," [0.3696752190589905, 0.3965989053249359, 0.42378437519073486],\n"," [0.391313761472702, 0.417245090007782, 0.4440343677997589],\n"," [0.40040871500968933, 0.4155190885066986, 0.419185608625412],\n"," [0.39773038029670715, 0.44195055961608887, 0.4510214030742645],\n"," [0.42276713252067566, 0.4375774562358856, 0.4424508213996887],\n"," [0.39659905433654785, 0.43255752325057983, 0.46513035893440247],\n"," [0.4043574333190918, 0.4196966588497162, 0.4240309000015259],\n"," [0.41504567861557007, 0.42874106764793396, 0.4322308599948883],\n"," [0.36938056349754333, 0.4060928523540497, 0.43880969285964966],\n"," [0.371614545583725, 0.4092506468296051, 0.4416586458683014],\n"," [0.4024035334587097, 0.4031979441642761, 0.426424115896225],\n"," [0.37443605065345764, 0.4108297526836395, 0.44177132844924927],\n"," [0.37776118516921997, 0.40242767333984375, 0.4325326383113861],\n"," [0.397417813539505, 0.39682474732398987, 0.40858379006385803],\n"," [0.3862689733505249, 0.41091737151145935, 0.4411225914955139],\n"," [0.37826165556907654, 0.41665494441986084, 0.4509488344192505],\n"," [0.4096905589103699, 0.4244106113910675, 0.4283524751663208],\n"," [0.3930046260356903, 0.42902886867523193, 0.4608186185359955],\n"," [0.39378175139427185, 0.42999571561813354, 0.462646484375],\n"," [0.37950795888900757, 0.40724971890449524, 0.43449950218200684],\n"," [0.42540523409843445, 0.4273935556411743, 0.43999427556991577],\n"," [0.39797601103782654, 0.41176047921180725, 0.41422802209854126],\n"," [0.39261624217033386, 0.40075036883354187, 0.416307657957077],\n"," [0.42286697030067444, 0.43656492233276367, 0.4405742585659027],\n"," [0.39567264914512634, 0.4205207824707031, 0.44696083664894104],\n"," [0.39889854192733765, 0.4135538637638092, 0.41848301887512207],\n"," [0.37233760952949524, 0.41367363929748535, 0.4216329753398895],\n"," [0.3853941559791565, 0.42953410744667053, 0.43806084990501404],\n"," [0.3928869962692261, 0.438355416059494, 0.4473000466823578],\n"," [0.38516730070114136, 0.4121415913105011, 0.43870481848716736],\n"," [0.39561334252357483, 0.41192498803138733, 0.4160878658294678],\n"," [0.35842153429985046, 0.3941606283187866, 0.42599233984947205],\n"," [0.4093225300312042, 0.4105130434036255, 0.43395575881004333],\n"," [0.38550493121147156, 0.4190623164176941, 0.4480234682559967],\n"," [0.38927459716796875, 0.4339495301246643, 0.4428573548793793],\n"," [0.3978646993637085, 0.42117348313331604, 0.44013673067092896],\n"," [0.39963018894195557, 0.401488333940506, 0.4259830117225647],\n"," [0.4095955789089203, 0.4101707339286804, 0.43301278352737427],\n"," [0.4138880670070648, 0.4279492497444153, 0.4312697947025299],\n"," [0.418369323015213, 0.43298301100730896, 0.4375889003276825],\n"," [0.4025056064128876, 0.4288886785507202, 0.46072664856910706],\n"," [0.39420226216316223, 0.43283984065055847, 0.4399023652076721],\n"," [0.3893830180168152, 0.4152241051197052, 0.4429064095020294],\n"," [0.41023749113082886, 0.4091864228248596, 0.4216523766517639],\n"," [0.3671405613422394, 0.41487541794776917, 0.4248201847076416],\n"," [0.38753634691238403, 0.410977840423584, 0.43036139011383057],\n"," [0.3907414972782135, 0.426680326461792, 0.4580804109573364],\n"," [0.4110064208507538, 0.4247909486293793, 0.42737719416618347],\n"," [0.37983283400535583, 0.4026666283607483, 0.4207715392112732],\n"," [0.40966111421585083, 0.4209481179714203, 0.42243650555610657],\n"," [0.4020896553993225, 0.4252547025680542, 0.44447216391563416],\n"," [0.3769325613975525, 0.413894385099411, 0.4472252130508423],\n"," [0.4093291163444519, 0.42310386896133423, 0.42739129066467285],\n"," [0.3814297616481781, 0.3904983699321747, 0.406573623418808],\n"," [0.40585261583328247, 0.4199451208114624, 0.42325451970100403],\n"," [0.4107571542263031, 0.4091425836086273, 0.42058247327804565],\n"," [0.4245765805244446, 0.42407166957855225, 0.44859686493873596],\n"," [0.43463975191116333, 0.4467715919017792, 0.4498019218444824],\n"," [0.35678914189338684, 0.38485074043273926, 0.412280797958374],\n"," [0.37398117780685425, 0.4169330298900604, 0.4255450963973999],\n"," [0.3897746801376343, 0.43448030948638916, 0.4437743127346039],\n"," [0.42443975806236267, 0.4392460286617279, 0.4444361627101898],\n"," [0.4041691720485687, 0.4192655384540558, 0.4236869812011719],\n"," [0.3608969449996948, 0.3980277180671692, 0.42964696884155273],\n"," [0.3798353970050812, 0.41623660922050476, 0.4474645256996155],\n"," [0.3904590904712677, 0.4255978465080261, 0.4580972492694855],\n"," [0.3943794369697571, 0.4156663715839386, 0.43307510018348694],\n"," [0.4125095307826996, 0.41263607144355774, 0.4373571574687958],\n"," [0.42276206612586975, 0.42397451400756836, 0.4495103061199188],\n"," [0.4088474214076996, 0.40854814648628235, 0.4324295222759247],\n"," [0.4154251217842102, 0.41371169686317444, 0.4253915250301361],\n"," [0.3884505331516266, 0.4222397804260254, 0.45275449752807617],\n"," [0.38122203946113586, 0.4181094765663147, 0.45127299427986145],\n"," [0.4086780846118927, 0.4140695035457611, 0.427489310503006],\n"," [0.3644578754901886, 0.3727126121520996, 0.3869529366493225],\n"," [0.3851173520088196, 0.39857912063598633, 0.40340229868888855],\n"," [0.3816441595554352, 0.40854692459106445, 0.4355517029762268],\n"," [0.3965660333633423, 0.4053601324558258, 0.4201364815235138],\n"," [0.38462355732917786, 0.4291192293167114, 0.43796274065971375],\n"," [0.4241715669631958, 0.4238797426223755, 0.4488907754421234],\n"," [0.4007694721221924, 0.40321287512779236, 0.4279332160949707],\n"," [0.3970228433609009, 0.4048631191253662, 0.41939201951026917],\n"," [0.36632126569747925, 0.40434935688972473, 0.4365260899066925],\n"," [0.4111976623535156, 0.42366641759872437, 0.42718467116355896],\n"," [0.4140060544013977, 0.42702725529670715, 0.42912474274635315],\n"," [0.40688300132751465, 0.4300977885723114, 0.45755648612976074],\n"," [0.37460005283355713, 0.42218688130378723, 0.4327014684677124],\n"," [0.3665221929550171, 0.4098418056964874, 0.41647323966026306],\n"," [0.4171578586101532, 0.41299307346343994, 0.4261615574359894],\n"," [0.42208337783813477, 0.4368170499801636, 0.4410761296749115],\n"," [0.39414846897125244, 0.40718701481819153, 0.4094325602054596],\n"," [0.3757927417755127, 0.41272661089897156, 0.44357118010520935],\n"," [0.3907160758972168, 0.42510151863098145, 0.4556073546409607],\n"," [0.41257748007774353, 0.4121876060962677, 0.4245736598968506],\n"," [0.3719538152217865, 0.41850051283836365, 0.4282042980194092],\n"," [0.417094886302948, 0.42200967669487, 0.4442434012889862],\n"," [0.3743583559989929, 0.39886435866355896, 0.4289146661758423],\n"," [0.35701945424079895, 0.383046954870224, 0.41414347290992737],\n"," [0.38923683762550354, 0.4254601001739502, 0.4576868414878845],\n"," [0.39791858196258545, 0.4019486606121063, 0.4174731373786926],\n"," [0.39526718854904175, 0.41986677050590515, 0.4480312764644623],\n"," [0.41376352310180664, 0.426081120967865, 0.42788466811180115],\n"," [0.41216278076171875, 0.42626357078552246, 0.4298843741416931],\n"," [0.3873685598373413, 0.4327499270439148, 0.4425645172595978],\n"," [0.39495033025741577, 0.42114904522895813, 0.45003530383110046],\n"," [0.38415002822875977, 0.40658366680145264, 0.4240732491016388],\n"," [0.37709474563598633, 0.40260517597198486, 0.4309171438217163],\n"," [0.4109775424003601, 0.4163784086704254, 0.41965940594673157],\n"," [0.36659178137779236, 0.4046015739440918, 0.436789333820343],\n"," [0.42445096373558044, 0.4385598599910736, 0.44299694895744324],\n"," [0.3621906638145447, 0.3926435708999634, 0.4209815561771393],\n"," [0.39918291568756104, 0.4233132302761078, 0.4496789574623108],\n"," [0.3773418664932251, 0.4130781590938568, 0.4447307586669922],\n"," [0.3759375214576721, 0.4041028320789337, 0.4320935606956482],\n"," [0.3721567690372467, 0.41877254843711853, 0.42794328927993774],\n"," [0.4166733920574188, 0.4175856411457062, 0.4395429193973541],\n"," [0.38366544246673584, 0.42629995942115784, 0.4336085021495819],\n"," [0.3996932804584503, 0.40030544996261597, 0.4231548011302948],\n"," [0.3987429141998291, 0.41433966159820557, 0.41943490505218506],\n"," [0.4102518558502197, 0.41166990995407104, 0.4370603859424591],\n"," [0.41571879386901855, 0.4300895929336548, 0.432667076587677],\n"," [0.41747140884399414, 0.4323767423629761, 0.43606120347976685],\n"," [0.4117697477340698, 0.42549586296081543, 0.4280841648578644],\n"," [0.38697370886802673, 0.42309269309043884, 0.45408210158348083],\n"," [0.3874751925468445, 0.4331352114677429, 0.4425815939903259],\n"," [0.4150894284248352, 0.41730114817619324, 0.443143367767334],\n"," [0.4126662313938141, 0.41144877672195435, 0.42304620146751404],\n"," [0.3798985481262207, 0.42367643117904663, 0.43295973539352417],\n"," [0.4143438935279846, 0.4149097204208374, 0.4397735595703125],\n"," [0.39271700382232666, 0.41764044761657715, 0.447292685508728],\n"," [0.4125954806804657, 0.4244192838668823, 0.45449548959732056],\n"," [0.3842242360115051, 0.4078642427921295, 0.4266122281551361],\n"," [0.41336190700531006, 0.4129757285118103, 0.4252074360847473],\n"," [0.3905658423900604, 0.41631147265434265, 0.4440784454345703],\n"," [0.4119570553302765, 0.4249056577682495, 0.4292476177215576],\n"," [0.41256025433540344, 0.4221673607826233, 0.42912784218788147],\n"," [0.3757648766040802, 0.41353967785835266, 0.44645237922668457],\n"," [0.42101606726646423, 0.42228707671165466, 0.447920560836792],\n"," [0.3925735652446747, 0.427084356546402, 0.4567210078239441],\n"," [0.4161703884601593, 0.41698187589645386, 0.442141592502594],\n"," [0.38269174098968506, 0.4252629578113556, 0.4321390986442566],\n"," [0.42221638560295105, 0.43482255935668945, 0.43747496604919434],\n"," [0.38804125785827637, 0.4218685030937195, 0.45546793937683105],\n"," [0.40995344519615173, 0.4103257656097412, 0.43367716670036316],\n"," [0.3732222616672516, 0.40296775102615356, 0.430542916059494],\n"," [0.38156330585479736, 0.4177854359149933, 0.4499070346355438],\n"," [0.4262414276599884, 0.4227946996688843, 0.4345889985561371],\n"," [0.3841300904750824, 0.4210679233074188, 0.4533495306968689],\n"," [0.41345417499542236, 0.41353416442871094, 0.4367503225803375],\n"," [0.3972364664077759, 0.41052496433258057, 0.41295701265335083],\n"," [0.3689984679222107, 0.4079754054546356, 0.4401705265045166],\n"," [0.3979136347770691, 0.42100420594215393, 0.44907209277153015],\n"," [0.3835577964782715, 0.4208850562572479, 0.451349675655365],\n"," [0.37560132145881653, 0.4016565978527069, 0.42752593755722046],\n"," [0.38595470786094666, 0.4218616485595703, 0.45267966389656067],\n"," [0.4175983667373657, 0.43117672204971313, 0.434378981590271],\n"," [0.400597482919693, 0.4086724817752838, 0.4242790639400482],\n"," [0.4167730510234833, 0.4148409068584442, 0.4260416626930237],\n"," [0.38752907514572144, 0.4125167429447174, 0.43914857506752014],\n"," [0.405950129032135, 0.4180639684200287, 0.42023965716362],\n"," [0.38475823402404785, 0.4085431694984436, 0.4380122125148773],\n"," [0.41514870524406433, 0.4289472997188568, 0.43188154697418213],\n"," [0.3838348090648651, 0.4194953441619873, 0.4504351019859314],\n"," [0.3713088631629944, 0.40709614753723145, 0.44132357835769653],\n"," [0.3819584846496582, 0.4078248143196106, 0.43534812331199646],\n"," [0.41003504395484924, 0.4108404517173767, 0.4357849061489105],\n"," [0.3827482759952545, 0.4185384511947632, 0.45020967721939087],\n"," [0.4315764904022217, 0.4315475523471832, 0.45632147789001465],\n"," [0.39936599135398865, 0.44248369336128235, 0.45122283697128296],\n"," [0.3930763602256775, 0.4192347228527069, 0.44752806425094604],\n"," [0.42132315039634705, 0.41956618428230286, 0.4326443672180176],\n"," [0.41234278678894043, 0.41288962960243225, 0.43740224838256836],\n"," [0.4119766056537628, 0.4261486530303955, 0.42737239599227905],\n"," [0.422115683555603, 0.43704429268836975, 0.46766161918640137],\n"," [0.4096362888813019, 0.4104560911655426, 0.4352790415287018],\n"," [0.39932742714881897, 0.41201528906822205, 0.4142566919326782],\n"," [0.40467604994773865, 0.40501704812049866, 0.41761770844459534],\n"," [0.4018506109714508, 0.4213728606700897, 0.4376299977302551],\n"," [0.395053505897522, 0.4308511018753052, 0.46202442049980164],\n"," [0.3721524775028229, 0.4093559682369232, 0.44248440861701965],\n"," [0.37612760066986084, 0.4240007996559143, 0.4346778988838196],\n"," [0.41960254311561584, 0.4290909171104431, 0.4303583800792694],\n"," [0.4008755683898926, 0.41462796926498413, 0.4175586700439453],\n"," [0.40725645422935486, 0.4219331443309784, 0.426902174949646],\n"," [0.3674388825893402, 0.3932762145996094, 0.4232221245765686],\n"," [0.40366965532302856, 0.40495479106903076, 0.42973387241363525],\n"," [0.3654440939426422, 0.39312833547592163, 0.42032039165496826],\n"," [0.42080050706863403, 0.4353731870651245, 0.43920156359672546],\n"," [0.3749171793460846, 0.41336262226104736, 0.44561436772346497],\n"," [0.3887394070625305, 0.41584256291389465, 0.44330012798309326],\n"," [0.3812759220600128, 0.4185791015625, 0.4503910541534424],\n"," [0.405304491519928, 0.4209654629230499, 0.4265284240245819],\n"," [0.40018168091773987, 0.39745697379112244, 0.41126975417137146],\n"," [0.40519213676452637, 0.40505900979042053, 0.4278485178947449],\n"," [0.40626171231269836, 0.40543314814567566, 0.4173329472541809],\n"," [0.4084911346435547, 0.42006853222846985, 0.422088623046875],\n"," [0.39244523644447327, 0.42907023429870605, 0.46211114525794983],\n"," [0.3927685618400574, 0.41355717182159424, 0.4305974245071411],\n"," [0.3788454234600067, 0.4055035710334778, 0.4333580434322357],\n"," [0.36945417523384094, 0.40752464532852173, 0.44037628173828125],\n"," ...]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["norm_mean=np.mean(means,axis =0)\n","norm_mean = list(norm_mean)\n","norm_mean"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iMzZUuOSWWed","outputId":"53a018f2-3ecc-48ae-d542-471acd371d98","executionInfo":{"status":"ok","timestamp":1670887478439,"user_tz":300,"elapsed":30,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.3959879955305368, 0.41699021958139393, 0.43522507327886295]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["norm_std=np.mean(stds,axis=0)\n","norm_std = list(norm_std)\n","norm_std"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SyTU6d88WYMW","outputId":"e06a02ce-c249-42c6-9b29-92af5a58453d","executionInfo":{"status":"ok","timestamp":1670887478439,"user_tz":300,"elapsed":7,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.21113784175510963, 0.1986706505394656, 0.1991702498400801]"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["# Train Dataset & Train loader"],"metadata":{"id":"Vq7EpxYh083G"}},{"cell_type":"code","source":["#transform=transforms.Compose([transforms.ToTensor()])\n","train_dataset = LazyLoadDataset(\"/content/\",transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       #transforms.RandomRotation(degrees=(0, 30)),\n","                       #transforms.ColorJitter(brightness=0.5, contrast=1, saturation=0.1, hue=0.5),\n","                       #transforms.RandomPerspective(distortion_scale=0.5, p=0.4),\n","                       transforms.Normalize(norm_mean, norm_std)])) "],"metadata":{"id":"Tw9RHT8WxqBB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(img_0, field_id), Y = train_dataset[0]"],"metadata":{"id":"7Ca3yNB4xw0q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_0.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RnZKPKFexwuZ","outputId":"a23ba79a-87d2-40e5-a919-195b5101eb9b","executionInfo":{"status":"ok","timestamp":1670887478439,"user_tz":300,"elapsed":6,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 224, 224])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["batch_size_train = 2\n","train_loader=DataLoader(train_dataset,batch_size=batch_size_train,shuffle=True)"],"metadata":{"id":"YhtDmDf_paWa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Y/1000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vN7_HFPdThMs","executionInfo":{"status":"ok","timestamp":1670887478439,"user_tz":300,"elapsed":5,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}},"outputId":"7ec13fa9-f1b0-4f2f-f0ea-f27f6cfd3c9e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.0522,  0.0528,  0.1172,  0.0645, -0.0028,  0.0988,  0.0699, -0.0515,\n","         0.0879,  0.0538,  0.0143, -0.0535])"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["# Test Dataset & Test loader"],"metadata":{"id":"dBLxVxhh1Avv"}},{"cell_type":"code","source":["test_dataset = LazyLoadDataset(\"/content/\",train=False,transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       #transforms.RandomRotation(degrees=(0, 30)),\n","                       #transforms.ColorJitter(brightness=0.5, contrast=1, saturation=0.1, hue=0.5),\n","                       #transforms.RandomPerspective(distortion_scale=0.5, p=0.4),\n","                       transforms.Normalize(norm_mean, norm_std),\n","                   ])) \n","\n","batch_size_test = 1\n","test_loader=DataLoader(test_dataset,batch_size=batch_size_test,shuffle=True)"],"metadata":{"id":"CxOFElBkyHIx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"w_LWFJe4E6hJ"}},{"cell_type":"code","source":["class Res(nn.Module):\n","    def __init__(self, input_channels, output_size):\n","        super(Res, self).__init__()\n","        \n","        # We use Sequential for simplicity\n","        self.stack = nn.Sequential(\n","            models.resnet50(pretrained=True),\n","            nn.Linear(1000, output_size)\n","        )                               \n","    def forward(self, x):\n","        x = self.stack(x)\n","        return x"],"metadata":{"id":"vY_bCsYCkLME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model input/output settings\n","input_channels = 3 # number of input channels\n","output_size=12"],"metadata":{"id":"PKKlmRW6W4LJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(epoch, model, optimizer,loader=train_loader):\n","    \"\"\"\n","    Train the model for one epoch\n","\n","    Args:\n","        epoch (int): current epoch\n","        model (nn.Module): model to train\n","        optimizer (torch.optim): optimizer to use\n","    \"\"\"\n","    model.train()\n","\n","    for batch_idx,((data, id),target) in enumerate(loader):\n","        # send to device\n","        data, target = data.to(device), target.to(device)\n","\n","        # consider passing different data augmentation in training\n","        # data = F.invert()\n","        # data = F.adjust_sharpness(data,sharpness_factor = 0.5)\n","        # data = F.adjust_hue(data,hue_factor = 0.9)\n","        # data = F.adjust_saturation(data,saturation_factor = 0.5)\n","        \n","        data = data.view(len(id),3,224, 224)\n","\n","        # make sure we erase all the gradients before computing new ones\n","        optimizer.zero_grad() \n","        \n","        # forward propagation\n","        output = model(data)\n","\n","        #RMSE loss\n","        loss = torch.sqrt(F.mse_loss(output,target))\n","        \n","        # backward propagation\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if batch_idx % 200 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(loader.dataset),\n","                100. * batch_idx / len(loader), loss))\n","    return loss"],"metadata":{"id":"Z0_gWSqmkTQe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test(model):\n","    \"\"\"\n","    Test the model\n","\n","    Args:\n","        model (nn.Module): model to test\n","      \n","    \"\"\"\n","    model.eval()\n","    ids = []\n","    preds = []\n","    with torch.no_grad():\n","      for batch_idx,(data, id) in enumerate(test_loader):\n","        # send to device\n","        data = data.to(device)          \n","        pred = model(data)\n","        for i in range(len(id)):\n","          ids.append(id[i])\n","          preds.append(np.array(pred[i].cpu()/1000,dtype=\"float64\"))\n","\n","    return ids, preds  "],"metadata":{"id":"eIReW5i30pDu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Cross-Validation & Grid-Search for determining hyperparameters"],"metadata":{"id":"RDHC5UCzdAIH"}},{"cell_type":"markdown","source":["What should hyperparameters in the models be? We could perform grid-search on the whole train dataset and pick the best-performing hyerparameters. But to avoid over-fitting on the train dataset, we split the whole train dataset into 9:1.\n","*   cross_val_train_set (90% of train dataset)\n","*   cross_val_test_set (10% of train dataset)\n","\n","We train our model on the cross_val_train_set and test our model on cross_val_test_set. We pick the best-performing hyperparameters on the cross_val_test_set.\n","\n","After all these steps, we train our model using the picked hyperparameters on the whole train dataset. "],"metadata":{"id":"Vnr1eVRBFpzY"}},{"cell_type":"code","source":["cross_val_train_set, cross_val_test_set = train_test_split(train_dataset, test_size=0.1)"],"metadata":{"id":"mDodEZ8WIDWq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sanity Check\n","len(cross_val_train_set) == int(len(train_dataset)*0.9)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W41pNa1jLbRu","executionInfo":{"status":"ok","timestamp":1670887491917,"user_tz":300,"elapsed":14,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}},"outputId":"02813a10-5e81-4ec7-bf3b-c1ab481211e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["batch_size_cross_val_train = 4\n","batch_size_cross_val_test = len(cross_val_test_set) "],"metadata":{"id":"HaA4CN_JMKR1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cross_val_train_loader=DataLoader(cross_val_train_set,batch_size=batch_size_cross_val_train,shuffle=True)\n","cross_val_test_loader=DataLoader(cross_val_test_set,batch_size=batch_size_cross_val_test,shuffle=True)"],"metadata":{"id":"H4jtAzwlfCDL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(cross_val_test_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QlpK2Fivid5w","executionInfo":{"status":"ok","timestamp":1670887491917,"user_tz":300,"elapsed":11,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}},"outputId":"43823232-3561-48b2-d3ef-50003746c1ed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["def cross_val(model):\n","    \"\"\"\n","    Test the model\n","\n","    Args:\n","        model (nn.Module): model to test\n","      \n","    \"\"\"\n","    model.eval()\n","    # We can just edit on the code for train(). We should delete the bakward propagation part and optimizer part.\n","\n","    with torch.no_grad():\n","      for batch_idx,((data, id),target) in enumerate(cross_val_test_loader):\n","          # send to device\n","          if batch_idx==0:\n","            data, target = data.to(device), target.to(device)\n","\n","            # forward propagation\n","            output = model(data)\n","\n","            #RMSE loss\n","            loss = torch.sqrt(F.mse_loss(output,target)) \n","            # Since batch_size_cross_val_test = len(cross_val_test_set), we have only one batch. So we can just return the loss on this batch.\n","            return loss.item()"],"metadata":{"id":"LPdxqOgLTkHI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def SGD_hyper_tuning(lr_list,momentum_list):  \n","  for j in momentum_list:\n","    for i in lr_list:\n","      model_res = Res(input_channels, output_size) # create Res model\n","      model_res.to(device)\n","      print(\"lr=\",i,\"momentum=\",j)\n","      train(epoch=0, model=model_res, optimizer=torch.optim.SGD(model_res.parameters(), lr=i,momentum=j),loader=cross_val_train_loader)\n","      print(\"Validation Performance:\",cross_val(model_res))"],"metadata":{"id":"28TGhoQ34Dag"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_list = [1e-5,1e-4,1e-3,1e-2,1e-1]\n","momentum_list = [0.99,0.97,0.95,0.93,0.91]\n","SGD_hyper_tuning(lr_list,momentum_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T5cJo0-WW-8J","executionInfo":{"status":"ok","timestamp":1670888544036,"user_tz":300,"elapsed":1041028,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}},"outputId":"63ddc717-373b-4e46-a20a-25d08e318d0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["lr= 1e-05 momentum= 0.99\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 69.606239\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 60.257511\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 23.980026\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 14.367318\n","Validation Performance: 13.263710975646973\n","lr= 0.0001 momentum= 0.99\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 64.843323\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 20.624750\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 10.833291\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 9.235901\n","Validation Performance: 7.795881271362305\n","lr= 0.001 momentum= 0.99\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 66.968620\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 16.725407\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 10.579671\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 10.512009\n","Validation Performance: 10.428595542907715\n","lr= 0.01 momentum= 0.99\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 69.055740\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 23.698336\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 16.356617\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 18.632277\n","Validation Performance: 15.773784637451172\n","lr= 0.1 momentum= 0.99\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 65.535255\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 29.153130\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 45.347195\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: nan\n","Validation Performance: nan\n","lr= 1e-05 momentum= 0.97\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 68.076584\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 58.932816\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 58.828396\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 39.134476\n","Validation Performance: 23.86100196838379\n","lr= 0.0001 momentum= 0.97\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 65.123772\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 14.887874\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 15.242998\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 14.545062\n","Validation Performance: 8.416244506835938\n","lr= 0.001 momentum= 0.97\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 62.714535\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 12.897777\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 9.107670\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 6.598476\n","Validation Performance: 10.035009384155273\n","lr= 0.01 momentum= 0.97\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 64.288803\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 20.566610\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 16.886198\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 15.006565\n","Validation Performance: 16.189674377441406\n","lr= 0.1 momentum= 0.97\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 70.443733\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 23.534630\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 29.597733\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 23.633905\n","Validation Performance: 25.329208374023438\n","lr= 1e-05 momentum= 0.95\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 66.450264\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 63.323437\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 61.773621\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 60.767906\n","Validation Performance: 50.94879150390625\n","lr= 0.0001 momentum= 0.95\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 66.389664\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 18.334158\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 11.602135\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 13.399168\n","Validation Performance: 8.552640914916992\n","lr= 0.001 momentum= 0.95\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 68.742722\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 12.823569\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 12.840055\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 7.134581\n","Validation Performance: 6.945775032043457\n","lr= 0.01 momentum= 0.95\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 72.360107\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 17.080486\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 11.104399\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 10.131621\n","Validation Performance: 9.886741638183594\n","lr= 0.1 momentum= 0.95\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 62.289040\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 22.202829\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 21.602434\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 13.988140\n","Validation Performance: 16.081586837768555\n","lr= 1e-05 momentum= 0.93\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 66.590256\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 68.226341\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 65.003845\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 63.773087\n","Validation Performance: 57.864444732666016\n","lr= 0.0001 momentum= 0.93\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 68.263786\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 20.401789\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 11.218000\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 15.468274\n","Validation Performance: 8.973008155822754\n","lr= 0.001 momentum= 0.93\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 70.454010\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 16.729124\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 9.071409\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 7.989297\n","Validation Performance: 6.932399749755859\n","lr= 0.01 momentum= 0.93\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 66.717064\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 18.978340\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 16.307697\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 12.536633\n","Validation Performance: 11.524869918823242\n","lr= 0.1 momentum= 0.93\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 63.513653\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 25.689011\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 16.865784\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 18.218014\n","Validation Performance: 14.752854347229004\n","lr= 1e-05 momentum= 0.91\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 70.410263\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 65.898582\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 66.515137\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 60.713142\n","Validation Performance: 60.029117584228516\n","lr= 0.0001 momentum= 0.91\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 68.866821\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 32.916439\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 18.447933\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 16.479467\n","Validation Performance: 9.372306823730469\n","lr= 0.001 momentum= 0.91\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 64.944763\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 16.526688\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 9.810483\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 8.129570\n","Validation Performance: 5.897029876708984\n","lr= 0.01 momentum= 0.91\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 66.613434\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 17.396194\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 11.838669\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 12.820840\n","Validation Performance: 9.97465705871582\n","lr= 0.1 momentum= 0.91\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 63.111866\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 27.182209\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 22.302689\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 21.911108\n","Validation Performance: 24.16677474975586\n"]}]},{"cell_type":"markdown","source":["\n","\n","Top 3 Performance\n","1.  lr = 1e-3, momentum = 0.91, loss on validation set = 5.897\n","2.  lr = 1e-3, momentum = 0.93, loss on validation set = 6.93\n","3.  lr = 1e-3, momentum = 0.95, loss on validation set = 6.94\n","\n","\n","\n"],"metadata":{"id":"xrQp2egcvE_4"}},{"cell_type":"code","source":["lr_list = [8e-4,9e-4,1e-3,2e-3,3e-3]\n","momentum_list = [0.99,0.97,0.95,0.93,0.91]\n","SGD_hyper_tuning(lr_list,momentum_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMwVkfsEuB-C","executionInfo":{"status":"ok","timestamp":1670890337507,"user_tz":300,"elapsed":1046818,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}},"outputId":"b7ac5061-baee-4a87-fd74-eface964047f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["lr= 0.0008 momentum= 0.99\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 68.818230\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 18.830072\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 17.464287\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 11.113073\n","Validation Performance: 13.585453987121582\n","lr= 0.0009 momentum= 0.99\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 67.563614\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 28.579670\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 16.316357\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 11.917153\n","Validation Performance: 13.811532020568848\n","lr= 0.001 momentum= 0.99\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 67.213844\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 15.255556\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 11.603078\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 9.428789\n","Validation Performance: 10.829007148742676\n","lr= 0.002 momentum= 0.99\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 69.599236\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 15.523223\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 14.475142\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 14.936378\n","Validation Performance: 14.18579387664795\n","lr= 0.003 momentum= 0.99\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 66.260757\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 19.255451\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 11.919391\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 14.168387\n","Validation Performance: 19.600557327270508\n","lr= 0.0008 momentum= 0.97\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 66.130043\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 9.380839\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 9.123262\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 10.286477\n","Validation Performance: 7.617082595825195\n","lr= 0.0009 momentum= 0.97\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 68.143921\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 8.190292\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 8.227539\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 7.510144\n","Validation Performance: 7.063784122467041\n","lr= 0.001 momentum= 0.97\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 64.956779\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 15.227592\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 10.870568\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 9.298900\n","Validation Performance: 7.547131538391113\n","lr= 0.002 momentum= 0.97\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 69.670143\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 14.812315\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 9.839427\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 7.115026\n","Validation Performance: 8.126799583435059\n","lr= 0.003 momentum= 0.97\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 64.428513\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 13.553739\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 17.089382\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 14.088470\n","Validation Performance: 11.71267032623291\n","lr= 0.0008 momentum= 0.95\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 62.953617\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 10.723756\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 8.367220\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 10.628306\n","Validation Performance: 5.754782676696777\n","lr= 0.0009 momentum= 0.95\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 67.055374\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 16.998207\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 11.616539\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 6.915567\n","Validation Performance: 6.255874156951904\n","lr= 0.001 momentum= 0.95\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 67.153732\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 10.192998\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 7.985791\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 8.622137\n","Validation Performance: 6.567749500274658\n","lr= 0.002 momentum= 0.95\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 63.710060\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 7.983236\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 7.577345\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 6.905286\n","Validation Performance: 6.9812517166137695\n","lr= 0.003 momentum= 0.95\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 66.762840\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 15.573871\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 9.636723\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 7.928262\n","Validation Performance: 8.662856101989746\n","lr= 0.0008 momentum= 0.93\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 67.651855\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 12.307584\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 9.583802\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 8.554613\n","Validation Performance: 7.469141960144043\n","lr= 0.0009 momentum= 0.93\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 65.335030\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 8.070685\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 7.498194\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 6.012694\n","Validation Performance: 7.740551471710205\n","lr= 0.001 momentum= 0.93\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 68.352623\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 18.086185\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 10.043626\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 5.055983\n","Validation Performance: 6.644536018371582\n","lr= 0.002 momentum= 0.93\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 66.693344\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 26.046854\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 13.617450\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 7.768711\n","Validation Performance: 7.0435075759887695\n","lr= 0.003 momentum= 0.93\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 65.310684\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 10.210646\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 10.825473\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 13.398345\n","Validation Performance: 7.88830041885376\n","lr= 0.0008 momentum= 0.91\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 64.572273\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 14.678323\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 14.117713\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 9.063449\n","Validation Performance: 7.690521240234375\n","lr= 0.0009 momentum= 0.91\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 64.931252\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 11.614872\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 7.759502\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 8.886865\n","Validation Performance: 5.8296613693237305\n","lr= 0.001 momentum= 0.91\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 67.003296\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 8.266639\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 10.791383\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 7.311921\n","Validation Performance: 7.090561866760254\n","lr= 0.002 momentum= 0.91\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 70.113617\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 12.612459\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 8.657924\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 7.998913\n","Validation Performance: 7.320501804351807\n","lr= 0.003 momentum= 0.91\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 65.543327\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 14.885177\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 8.664802\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 11.167387\n","Validation Performance: 5.72755241394043\n"]}]},{"cell_type":"markdown","source":["Top 5 Performance\n","\n","1.   lr = 3e-3, momentum = 0.91, loss on validation set = 5.72\n","1.   lr = 8e-4, momentum = 0.95, loss on validation set = 5.75\n","2.   lr = 9e-4, momentum = 0.91, loss on validation set = 5.82\n","\n","3.   lr = 9e-4, momentum = 0.95, loss on validation set = 6.25\n","\n","4.   lr = 1e-3, momentum = 0.95, loss on validation set = 6.56"],"metadata":{"id":"IXlOu7KXyw7H"}},{"cell_type":"code","source":["## 故意放置的错误，记得删掉\n","lr_list = [1e-3,2e-3,3e-3,4e-3,5e-3]\n","momentum_list = [0.99,0.97,0.95,0.93,0.91]\n","for j in momentum_list:\n","  for i in lr_list:\n","    model_res = Res(input_channels, output_size) # create Res model\n","    model_res.to(device)\n","    optimizer = torch.optim.SGD(model_res.parameters(), lr=i,momentum=j) \n","    print(\"lr=\",i,\"momentum=\",j)\n","    train(epoch=1, model=model_res, optimizer=optimizer,stop_at_batch_idx=30)"],"metadata":{"id":"vP-fqwqDQ7tm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# lr=8e-4, momentum=0.95, 10 epoch, train_batch_size = 2, no lr decay"],"metadata":{"id":"SKaRo02pLMLq"}},{"cell_type":"code","source":["model_res = Res(input_channels, output_size) # create Res model\n","model_res.to(device)\n","optimizer = torch.optim.SGD(model_res.parameters(), lr=8e-4,momentum=0.95) \n","\n","print(\"lr=\",8e-4,\"momentum=\",0.95)\n","for epoch in range(10):\n","  train(epoch, model=model_res, optimizer=torch.optim.SGD(model_res.parameters(), lr=8e-4,momentum=0.95),loader=train_loader)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D4x_-B6vLvNS","outputId":"d098b9f8-5282-45f2-b8c7-0a2b25d6d9c4","executionInfo":{"status":"ok","timestamp":1670892640653,"user_tz":300,"elapsed":680703,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["lr= 0.0008 momentum= 0.95\n","Train Epoch: 0 [0/3396 (0%)]\tLoss: 67.233040\n","Train Epoch: 0 [400/3396 (12%)]\tLoss: 11.448866\n","Train Epoch: 0 [800/3396 (24%)]\tLoss: 17.135019\n","Train Epoch: 0 [1200/3396 (35%)]\tLoss: 11.790951\n","Train Epoch: 0 [1600/3396 (47%)]\tLoss: 7.063645\n","Train Epoch: 0 [2000/3396 (59%)]\tLoss: 6.542379\n","Train Epoch: 0 [2400/3396 (71%)]\tLoss: 8.619304\n","Train Epoch: 0 [2800/3396 (82%)]\tLoss: 7.867995\n","Train Epoch: 0 [3200/3396 (94%)]\tLoss: 7.449847\n","Train Epoch: 1 [0/3396 (0%)]\tLoss: 5.336625\n","Train Epoch: 1 [400/3396 (12%)]\tLoss: 6.257270\n","Train Epoch: 1 [800/3396 (24%)]\tLoss: 5.208410\n","Train Epoch: 1 [1200/3396 (35%)]\tLoss: 3.927015\n","Train Epoch: 1 [1600/3396 (47%)]\tLoss: 7.297187\n","Train Epoch: 1 [2000/3396 (59%)]\tLoss: 4.772007\n","Train Epoch: 1 [2400/3396 (71%)]\tLoss: 5.784763\n","Train Epoch: 1 [2800/3396 (82%)]\tLoss: 4.739461\n","Train Epoch: 1 [3200/3396 (94%)]\tLoss: 3.224008\n","Train Epoch: 2 [0/3396 (0%)]\tLoss: 5.617990\n","Train Epoch: 2 [400/3396 (12%)]\tLoss: 4.559106\n","Train Epoch: 2 [800/3396 (24%)]\tLoss: 3.215220\n","Train Epoch: 2 [1200/3396 (35%)]\tLoss: 4.410352\n","Train Epoch: 2 [1600/3396 (47%)]\tLoss: 3.641047\n","Train Epoch: 2 [2000/3396 (59%)]\tLoss: 4.001399\n","Train Epoch: 2 [2400/3396 (71%)]\tLoss: 3.240063\n","Train Epoch: 2 [2800/3396 (82%)]\tLoss: 5.749955\n","Train Epoch: 2 [3200/3396 (94%)]\tLoss: 5.722305\n","Train Epoch: 3 [0/3396 (0%)]\tLoss: 4.017798\n","Train Epoch: 3 [400/3396 (12%)]\tLoss: 3.678348\n","Train Epoch: 3 [800/3396 (24%)]\tLoss: 5.015151\n","Train Epoch: 3 [1200/3396 (35%)]\tLoss: 4.213551\n","Train Epoch: 3 [1600/3396 (47%)]\tLoss: 2.859036\n","Train Epoch: 3 [2000/3396 (59%)]\tLoss: 5.988614\n","Train Epoch: 3 [2400/3396 (71%)]\tLoss: 3.940962\n","Train Epoch: 3 [2800/3396 (82%)]\tLoss: 2.884243\n","Train Epoch: 3 [3200/3396 (94%)]\tLoss: 3.784050\n","Train Epoch: 4 [0/3396 (0%)]\tLoss: 3.762421\n","Train Epoch: 4 [400/3396 (12%)]\tLoss: 3.717678\n","Train Epoch: 4 [800/3396 (24%)]\tLoss: 4.801144\n","Train Epoch: 4 [1200/3396 (35%)]\tLoss: 4.132370\n","Train Epoch: 4 [1600/3396 (47%)]\tLoss: 2.528512\n","Train Epoch: 4 [2000/3396 (59%)]\tLoss: 4.969519\n","Train Epoch: 4 [2400/3396 (71%)]\tLoss: 4.574998\n","Train Epoch: 4 [2800/3396 (82%)]\tLoss: 3.945304\n","Train Epoch: 4 [3200/3396 (94%)]\tLoss: 1.915831\n","Train Epoch: 5 [0/3396 (0%)]\tLoss: 3.723686\n","Train Epoch: 5 [400/3396 (12%)]\tLoss: 3.594567\n","Train Epoch: 5 [800/3396 (24%)]\tLoss: 3.560264\n","Train Epoch: 5 [1200/3396 (35%)]\tLoss: 5.984110\n","Train Epoch: 5 [1600/3396 (47%)]\tLoss: 3.937720\n","Train Epoch: 5 [2000/3396 (59%)]\tLoss: 2.945356\n","Train Epoch: 5 [2400/3396 (71%)]\tLoss: 2.171941\n","Train Epoch: 5 [2800/3396 (82%)]\tLoss: 3.895408\n","Train Epoch: 5 [3200/3396 (94%)]\tLoss: 2.701957\n","Train Epoch: 6 [0/3396 (0%)]\tLoss: 2.810712\n","Train Epoch: 6 [400/3396 (12%)]\tLoss: 2.067762\n","Train Epoch: 6 [800/3396 (24%)]\tLoss: 2.483491\n","Train Epoch: 6 [1200/3396 (35%)]\tLoss: 3.818291\n","Train Epoch: 6 [1600/3396 (47%)]\tLoss: 1.907798\n","Train Epoch: 6 [2000/3396 (59%)]\tLoss: 4.664429\n","Train Epoch: 6 [2400/3396 (71%)]\tLoss: 1.868626\n","Train Epoch: 6 [2800/3396 (82%)]\tLoss: 3.406617\n","Train Epoch: 6 [3200/3396 (94%)]\tLoss: 2.436838\n","Train Epoch: 7 [0/3396 (0%)]\tLoss: 1.794346\n","Train Epoch: 7 [400/3396 (12%)]\tLoss: 2.045362\n","Train Epoch: 7 [800/3396 (24%)]\tLoss: 2.916559\n","Train Epoch: 7 [1200/3396 (35%)]\tLoss: 3.439077\n","Train Epoch: 7 [1600/3396 (47%)]\tLoss: 3.906060\n","Train Epoch: 7 [2000/3396 (59%)]\tLoss: 3.510396\n","Train Epoch: 7 [2400/3396 (71%)]\tLoss: 2.351252\n","Train Epoch: 7 [2800/3396 (82%)]\tLoss: 2.551536\n","Train Epoch: 7 [3200/3396 (94%)]\tLoss: 2.934324\n","Train Epoch: 8 [0/3396 (0%)]\tLoss: 2.212528\n","Train Epoch: 8 [400/3396 (12%)]\tLoss: 3.108292\n","Train Epoch: 8 [800/3396 (24%)]\tLoss: 1.863292\n","Train Epoch: 8 [1200/3396 (35%)]\tLoss: 3.064892\n","Train Epoch: 8 [1600/3396 (47%)]\tLoss: 2.618779\n","Train Epoch: 8 [2000/3396 (59%)]\tLoss: 3.476274\n","Train Epoch: 8 [2400/3396 (71%)]\tLoss: 1.347039\n","Train Epoch: 8 [2800/3396 (82%)]\tLoss: 3.484177\n","Train Epoch: 8 [3200/3396 (94%)]\tLoss: 3.453269\n","Train Epoch: 9 [0/3396 (0%)]\tLoss: 4.003882\n","Train Epoch: 9 [400/3396 (12%)]\tLoss: 3.612497\n","Train Epoch: 9 [800/3396 (24%)]\tLoss: 2.096999\n","Train Epoch: 9 [1200/3396 (35%)]\tLoss: 2.228299\n","Train Epoch: 9 [1600/3396 (47%)]\tLoss: 2.551790\n","Train Epoch: 9 [2000/3396 (59%)]\tLoss: 2.328278\n","Train Epoch: 9 [2400/3396 (71%)]\tLoss: 3.002708\n","Train Epoch: 9 [2800/3396 (82%)]\tLoss: 3.659362\n","Train Epoch: 9 [3200/3396 (94%)]\tLoss: 3.415997\n"]}]},{"cell_type":"code","source":["torch.save(model_res,'/content/drive/MyDrive/ML_Capstone/project_v7_res_img0(fast)_00008095.pt')\n","model_res = torch.load('/content/drive/MyDrive/ML_Capstone/project_v7_res_img0(fast)_00008095.pt')\n","model_res.eval()\n","ids,preds = test(model_res)\n","\n","outfile = '/content/drive/MyDrive/ML_Capstone/submission_v7_img0(fast)_00008095.csv'\n","\n","output_file = open(outfile, 'w')\n","\n","titles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n","         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\n","\n","df = pd.concat([pd.DataFrame(ids), pd.DataFrame.from_records(preds)], axis = 1, names = titles)\n","df.columns = titles\n","df.to_csv(outfile, index = False)\n","print(\"Written to csv file {}\".format(outfile))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"As8keaSnEbAG","executionInfo":{"status":"ok","timestamp":1670894495605,"user_tz":300,"elapsed":12612,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}},"outputId":"a67fdb29-48a9-40d3-f3c0-6a20ff0dabbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Written to csv file /content/drive/MyDrive/ML_Capstone/submission_v7_img0(fast)_00004095.csv\n"]}]},{"cell_type":"markdown","source":["#lr=3e-3, momentum=0.91, 10 epoch, train_batch_size = 2, with lr decay"],"metadata":{"id":"rU_20PkvLxdX"}},{"cell_type":"code","source":["model_res = Res(input_channels, output_size) # create Res model\n","model_res.to(device)\n","optimizer = torch.optim.SGD(model_res.parameters(), lr=3e-3,momentum=0.91) \n","scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n","\n","print(\"lr=\",3e-3,\"momentum=\",0.91)\n","for epoch in range(10):\n","  train(epoch, model=model_res, optimizer=torch.optim.SGD(model_res.parameters(), lr=3e-3,momentum=0.91),loader=train_loader)\n","  scheduler.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bEwyLTiQmGct","executionInfo":{"status":"ok","timestamp":1670891676374,"user_tz":300,"elapsed":690152,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}},"outputId":"bbac6b6f-0e93-4db1-daa7-e196e2c32213"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["lr= 0.003 momentum= 0.91\n","Train Epoch: 0 [0/3396 (0%)]\tLoss: 67.939072\n","Train Epoch: 0 [400/3396 (12%)]\tLoss: 13.923128\n","Train Epoch: 0 [800/3396 (24%)]\tLoss: 13.405359\n","Train Epoch: 0 [1200/3396 (35%)]\tLoss: 9.460212\n","Train Epoch: 0 [1600/3396 (47%)]\tLoss: 10.889026\n","Train Epoch: 0 [2000/3396 (59%)]\tLoss: 8.984818\n","Train Epoch: 0 [2400/3396 (71%)]\tLoss: 10.970617\n","Train Epoch: 0 [2800/3396 (82%)]\tLoss: 8.084895\n","Train Epoch: 0 [3200/3396 (94%)]\tLoss: 6.002130\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"]},{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/3396 (0%)]\tLoss: 8.923368\n","Train Epoch: 1 [400/3396 (12%)]\tLoss: 6.965173\n","Train Epoch: 1 [800/3396 (24%)]\tLoss: 5.476079\n","Train Epoch: 1 [1200/3396 (35%)]\tLoss: 7.721508\n","Train Epoch: 1 [1600/3396 (47%)]\tLoss: 4.278842\n","Train Epoch: 1 [2000/3396 (59%)]\tLoss: 6.796082\n","Train Epoch: 1 [2400/3396 (71%)]\tLoss: 7.104254\n","Train Epoch: 1 [2800/3396 (82%)]\tLoss: 5.075162\n","Train Epoch: 1 [3200/3396 (94%)]\tLoss: 9.091908\n","Train Epoch: 2 [0/3396 (0%)]\tLoss: 4.974702\n","Train Epoch: 2 [400/3396 (12%)]\tLoss: 3.828071\n","Train Epoch: 2 [800/3396 (24%)]\tLoss: 4.713331\n","Train Epoch: 2 [1200/3396 (35%)]\tLoss: 5.700161\n","Train Epoch: 2 [1600/3396 (47%)]\tLoss: 6.056094\n","Train Epoch: 2 [2000/3396 (59%)]\tLoss: 4.684329\n","Train Epoch: 2 [2400/3396 (71%)]\tLoss: 4.639878\n","Train Epoch: 2 [2800/3396 (82%)]\tLoss: 3.255405\n","Train Epoch: 2 [3200/3396 (94%)]\tLoss: 3.079749\n","Train Epoch: 3 [0/3396 (0%)]\tLoss: 4.688659\n","Train Epoch: 3 [400/3396 (12%)]\tLoss: 2.683493\n","Train Epoch: 3 [800/3396 (24%)]\tLoss: 4.055069\n","Train Epoch: 3 [1200/3396 (35%)]\tLoss: 4.440053\n","Train Epoch: 3 [1600/3396 (47%)]\tLoss: 4.021105\n","Train Epoch: 3 [2000/3396 (59%)]\tLoss: 3.166186\n","Train Epoch: 3 [2400/3396 (71%)]\tLoss: 3.300512\n","Train Epoch: 3 [2800/3396 (82%)]\tLoss: 5.277757\n","Train Epoch: 3 [3200/3396 (94%)]\tLoss: 4.154082\n","Train Epoch: 4 [0/3396 (0%)]\tLoss: 4.567986\n","Train Epoch: 4 [400/3396 (12%)]\tLoss: 2.889269\n","Train Epoch: 4 [800/3396 (24%)]\tLoss: 3.843762\n","Train Epoch: 4 [1200/3396 (35%)]\tLoss: 3.838758\n","Train Epoch: 4 [1600/3396 (47%)]\tLoss: 3.656227\n","Train Epoch: 4 [2000/3396 (59%)]\tLoss: 3.787651\n","Train Epoch: 4 [2400/3396 (71%)]\tLoss: 5.165689\n","Train Epoch: 4 [2800/3396 (82%)]\tLoss: 3.668158\n","Train Epoch: 4 [3200/3396 (94%)]\tLoss: 3.578320\n","Train Epoch: 5 [0/3396 (0%)]\tLoss: 4.099117\n","Train Epoch: 5 [400/3396 (12%)]\tLoss: 3.031804\n","Train Epoch: 5 [800/3396 (24%)]\tLoss: 2.288841\n","Train Epoch: 5 [1200/3396 (35%)]\tLoss: 5.963245\n","Train Epoch: 5 [1600/3396 (47%)]\tLoss: 2.440520\n","Train Epoch: 5 [2000/3396 (59%)]\tLoss: 3.092609\n","Train Epoch: 5 [2400/3396 (71%)]\tLoss: 2.809376\n","Train Epoch: 5 [2800/3396 (82%)]\tLoss: 3.737800\n","Train Epoch: 5 [3200/3396 (94%)]\tLoss: 2.803936\n","Train Epoch: 6 [0/3396 (0%)]\tLoss: 5.878402\n","Train Epoch: 6 [400/3396 (12%)]\tLoss: 3.783716\n","Train Epoch: 6 [800/3396 (24%)]\tLoss: 4.121475\n","Train Epoch: 6 [1200/3396 (35%)]\tLoss: 3.366356\n","Train Epoch: 6 [1600/3396 (47%)]\tLoss: 3.842722\n","Train Epoch: 6 [2000/3396 (59%)]\tLoss: 6.041355\n","Train Epoch: 6 [2400/3396 (71%)]\tLoss: 3.891362\n","Train Epoch: 6 [2800/3396 (82%)]\tLoss: 2.818425\n","Train Epoch: 6 [3200/3396 (94%)]\tLoss: 3.539636\n","Train Epoch: 7 [0/3396 (0%)]\tLoss: 2.788293\n","Train Epoch: 7 [400/3396 (12%)]\tLoss: 3.111228\n","Train Epoch: 7 [800/3396 (24%)]\tLoss: 2.268161\n","Train Epoch: 7 [1200/3396 (35%)]\tLoss: 3.480230\n","Train Epoch: 7 [1600/3396 (47%)]\tLoss: 2.427110\n","Train Epoch: 7 [2000/3396 (59%)]\tLoss: 3.944405\n","Train Epoch: 7 [2400/3396 (71%)]\tLoss: 4.139801\n","Train Epoch: 7 [2800/3396 (82%)]\tLoss: 4.168025\n","Train Epoch: 7 [3200/3396 (94%)]\tLoss: 1.219737\n","Train Epoch: 8 [0/3396 (0%)]\tLoss: 1.626897\n","Train Epoch: 8 [400/3396 (12%)]\tLoss: 2.777187\n","Train Epoch: 8 [800/3396 (24%)]\tLoss: 2.364275\n","Train Epoch: 8 [1200/3396 (35%)]\tLoss: 2.478834\n","Train Epoch: 8 [1600/3396 (47%)]\tLoss: 1.899534\n","Train Epoch: 8 [2000/3396 (59%)]\tLoss: 3.506932\n","Train Epoch: 8 [2400/3396 (71%)]\tLoss: 3.991173\n","Train Epoch: 8 [2800/3396 (82%)]\tLoss: 2.666845\n","Train Epoch: 8 [3200/3396 (94%)]\tLoss: 2.216550\n","Train Epoch: 9 [0/3396 (0%)]\tLoss: 1.968956\n","Train Epoch: 9 [400/3396 (12%)]\tLoss: 2.838279\n","Train Epoch: 9 [800/3396 (24%)]\tLoss: 2.121512\n","Train Epoch: 9 [1200/3396 (35%)]\tLoss: 3.293035\n","Train Epoch: 9 [1600/3396 (47%)]\tLoss: 2.602337\n","Train Epoch: 9 [2000/3396 (59%)]\tLoss: 3.547949\n","Train Epoch: 9 [2400/3396 (71%)]\tLoss: 2.468647\n","Train Epoch: 9 [2800/3396 (82%)]\tLoss: 2.858099\n","Train Epoch: 9 [3200/3396 (94%)]\tLoss: 2.572736\n"]}]},{"cell_type":"code","source":["torch.save(model_res,'/content/drive/MyDrive/ML_Capstone/project_v7_res_img0(fast)_0003091.pt')\n","model_res = torch.load('/content/drive/MyDrive/ML_Capstone/project_v7_res_img0(fast)_0003091.pt')\n","model_res.eval()\n","ids,preds = test(model_res)\n","\n","\n","\n","outfile = '/content/drive/MyDrive/ML_Capstone/submission_v7_img0(fast)_0003091.csv'\n","\n","output_file = open(outfile, 'w')\n","\n","titles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n","         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\n","\n","df = pd.concat([pd.DataFrame(ids), pd.DataFrame.from_records(preds)], axis = 1, names = titles)\n","df.columns = titles\n","df.to_csv(outfile, index = False)\n","print(\"Written to csv file {}\".format(outfile))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d6tTAwdrKuqX","executionInfo":{"status":"ok","timestamp":1670891804207,"user_tz":300,"elapsed":13843,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}},"outputId":"7f085402-9fcc-404d-813b-035bc43d351f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Written to csv file /content/drive/MyDrive/ML_Capstone/submission_v7_img0(fast)_0003091.csv\n"]}]},{"cell_type":"markdown","source":["# lr=3e-3, momentum=0.91, 20 epoch, train_batch_size = 2, no lr decay"],"metadata":{"id":"WzM3Q_lXL_VX"}},{"cell_type":"code","source":["model_res = Res(input_channels, output_size) # create Res model\n","model_res.to(device)\n","optimizer = torch.optim.SGD(model_res.parameters(), lr=3e-3,momentum=0.91) \n","\n","print(\"lr=\",3e-3,\"momentum=\",0.91)\n","for epoch in range(20):\n","  train(epoch, model=model_res, optimizer=torch.optim.SGD(model_res.parameters(), lr=3e-3,momentum=0.91),loader=train_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oSyGFNgdFAKY","executionInfo":{"status":"ok","timestamp":1670895916438,"user_tz":300,"elapsed":1314475,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}},"outputId":"9b920dd0-c734-4716-b02e-2dc223bbd081"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["lr= 0.003 momentum= 0.91\n","Train Epoch: 0 [0/3396 (0%)]\tLoss: 65.925781\n","Train Epoch: 0 [400/3396 (12%)]\tLoss: 24.333570\n","Train Epoch: 0 [800/3396 (24%)]\tLoss: 9.356822\n","Train Epoch: 0 [1200/3396 (35%)]\tLoss: 10.962144\n","Train Epoch: 0 [1600/3396 (47%)]\tLoss: 11.152804\n","Train Epoch: 0 [2000/3396 (59%)]\tLoss: 12.381385\n","Train Epoch: 0 [2400/3396 (71%)]\tLoss: 10.082131\n","Train Epoch: 0 [2800/3396 (82%)]\tLoss: 9.438674\n","Train Epoch: 0 [3200/3396 (94%)]\tLoss: 6.253980\n","Train Epoch: 1 [0/3396 (0%)]\tLoss: 8.764084\n","Train Epoch: 1 [400/3396 (12%)]\tLoss: 6.879309\n","Train Epoch: 1 [800/3396 (24%)]\tLoss: 4.917552\n","Train Epoch: 1 [1200/3396 (35%)]\tLoss: 6.809510\n","Train Epoch: 1 [1600/3396 (47%)]\tLoss: 8.005464\n","Train Epoch: 1 [2000/3396 (59%)]\tLoss: 7.180151\n","Train Epoch: 1 [2400/3396 (71%)]\tLoss: 5.265434\n","Train Epoch: 1 [2800/3396 (82%)]\tLoss: 7.302955\n","Train Epoch: 1 [3200/3396 (94%)]\tLoss: 4.902833\n","Train Epoch: 2 [0/3396 (0%)]\tLoss: 7.030436\n","Train Epoch: 2 [400/3396 (12%)]\tLoss: 6.550316\n","Train Epoch: 2 [800/3396 (24%)]\tLoss: 6.503278\n","Train Epoch: 2 [1200/3396 (35%)]\tLoss: 4.876726\n","Train Epoch: 2 [1600/3396 (47%)]\tLoss: 3.339262\n","Train Epoch: 2 [2000/3396 (59%)]\tLoss: 4.739234\n","Train Epoch: 2 [2400/3396 (71%)]\tLoss: 3.665295\n","Train Epoch: 2 [2800/3396 (82%)]\tLoss: 7.785617\n","Train Epoch: 2 [3200/3396 (94%)]\tLoss: 3.817398\n","Train Epoch: 3 [0/3396 (0%)]\tLoss: 4.467444\n","Train Epoch: 3 [400/3396 (12%)]\tLoss: 3.142345\n","Train Epoch: 3 [800/3396 (24%)]\tLoss: 5.544332\n","Train Epoch: 3 [1200/3396 (35%)]\tLoss: 2.454110\n","Train Epoch: 3 [1600/3396 (47%)]\tLoss: 4.948283\n","Train Epoch: 3 [2000/3396 (59%)]\tLoss: 3.138656\n","Train Epoch: 3 [2400/3396 (71%)]\tLoss: 4.817176\n","Train Epoch: 3 [2800/3396 (82%)]\tLoss: 4.203883\n","Train Epoch: 3 [3200/3396 (94%)]\tLoss: 5.185837\n","Train Epoch: 4 [0/3396 (0%)]\tLoss: 4.397951\n","Train Epoch: 4 [400/3396 (12%)]\tLoss: 4.564981\n","Train Epoch: 4 [800/3396 (24%)]\tLoss: 3.191135\n","Train Epoch: 4 [1200/3396 (35%)]\tLoss: 4.375721\n","Train Epoch: 4 [1600/3396 (47%)]\tLoss: 5.998106\n","Train Epoch: 4 [2000/3396 (59%)]\tLoss: 3.759645\n","Train Epoch: 4 [2400/3396 (71%)]\tLoss: 4.734010\n","Train Epoch: 4 [2800/3396 (82%)]\tLoss: 5.316019\n","Train Epoch: 4 [3200/3396 (94%)]\tLoss: 4.051806\n","Train Epoch: 5 [0/3396 (0%)]\tLoss: 5.772557\n","Train Epoch: 5 [400/3396 (12%)]\tLoss: 3.080959\n","Train Epoch: 5 [800/3396 (24%)]\tLoss: 2.826087\n","Train Epoch: 5 [1200/3396 (35%)]\tLoss: 3.927032\n","Train Epoch: 5 [1600/3396 (47%)]\tLoss: 2.992212\n","Train Epoch: 5 [2000/3396 (59%)]\tLoss: 4.290910\n","Train Epoch: 5 [2400/3396 (71%)]\tLoss: 4.111809\n","Train Epoch: 5 [2800/3396 (82%)]\tLoss: 4.837953\n","Train Epoch: 5 [3200/3396 (94%)]\tLoss: 3.185243\n","Train Epoch: 6 [0/3396 (0%)]\tLoss: 4.217093\n","Train Epoch: 6 [400/3396 (12%)]\tLoss: 2.412621\n","Train Epoch: 6 [800/3396 (24%)]\tLoss: 3.708629\n","Train Epoch: 6 [1200/3396 (35%)]\tLoss: 3.469288\n","Train Epoch: 6 [1600/3396 (47%)]\tLoss: 3.818780\n","Train Epoch: 6 [2000/3396 (59%)]\tLoss: 2.694838\n","Train Epoch: 6 [2400/3396 (71%)]\tLoss: 3.103196\n","Train Epoch: 6 [2800/3396 (82%)]\tLoss: 3.124835\n","Train Epoch: 6 [3200/3396 (94%)]\tLoss: 3.223380\n","Train Epoch: 7 [0/3396 (0%)]\tLoss: 3.207907\n","Train Epoch: 7 [400/3396 (12%)]\tLoss: 3.107608\n","Train Epoch: 7 [800/3396 (24%)]\tLoss: 3.032151\n","Train Epoch: 7 [1200/3396 (35%)]\tLoss: 2.876837\n","Train Epoch: 7 [1600/3396 (47%)]\tLoss: 3.816072\n","Train Epoch: 7 [2000/3396 (59%)]\tLoss: 2.227002\n","Train Epoch: 7 [2400/3396 (71%)]\tLoss: 3.409257\n","Train Epoch: 7 [2800/3396 (82%)]\tLoss: 3.097818\n","Train Epoch: 7 [3200/3396 (94%)]\tLoss: 2.404853\n","Train Epoch: 8 [0/3396 (0%)]\tLoss: 2.318011\n","Train Epoch: 8 [400/3396 (12%)]\tLoss: 3.217982\n","Train Epoch: 8 [800/3396 (24%)]\tLoss: 1.774508\n","Train Epoch: 8 [1200/3396 (35%)]\tLoss: 3.162544\n","Train Epoch: 8 [1600/3396 (47%)]\tLoss: 3.245758\n","Train Epoch: 8 [2000/3396 (59%)]\tLoss: 2.936761\n","Train Epoch: 8 [2400/3396 (71%)]\tLoss: 7.366680\n","Train Epoch: 8 [2800/3396 (82%)]\tLoss: 3.350965\n","Train Epoch: 8 [3200/3396 (94%)]\tLoss: 3.078295\n","Train Epoch: 9 [0/3396 (0%)]\tLoss: 4.977693\n","Train Epoch: 9 [400/3396 (12%)]\tLoss: 2.148546\n","Train Epoch: 9 [800/3396 (24%)]\tLoss: 3.939877\n","Train Epoch: 9 [1200/3396 (35%)]\tLoss: 3.220016\n","Train Epoch: 9 [1600/3396 (47%)]\tLoss: 2.362961\n","Train Epoch: 9 [2000/3396 (59%)]\tLoss: 2.741580\n","Train Epoch: 9 [2400/3396 (71%)]\tLoss: 3.270123\n","Train Epoch: 9 [2800/3396 (82%)]\tLoss: 3.438148\n","Train Epoch: 9 [3200/3396 (94%)]\tLoss: 4.376807\n","Train Epoch: 10 [0/3396 (0%)]\tLoss: 3.674533\n","Train Epoch: 10 [400/3396 (12%)]\tLoss: 2.311893\n","Train Epoch: 10 [800/3396 (24%)]\tLoss: 2.025441\n","Train Epoch: 10 [1200/3396 (35%)]\tLoss: 2.949565\n","Train Epoch: 10 [1600/3396 (47%)]\tLoss: 2.359823\n","Train Epoch: 10 [2000/3396 (59%)]\tLoss: 5.258461\n","Train Epoch: 10 [2400/3396 (71%)]\tLoss: 3.803903\n","Train Epoch: 10 [2800/3396 (82%)]\tLoss: 2.647262\n","Train Epoch: 10 [3200/3396 (94%)]\tLoss: 3.988879\n","Train Epoch: 11 [0/3396 (0%)]\tLoss: 3.237667\n","Train Epoch: 11 [400/3396 (12%)]\tLoss: 2.660643\n","Train Epoch: 11 [800/3396 (24%)]\tLoss: 2.869395\n","Train Epoch: 11 [1200/3396 (35%)]\tLoss: 3.927652\n","Train Epoch: 11 [1600/3396 (47%)]\tLoss: 3.904931\n","Train Epoch: 11 [2000/3396 (59%)]\tLoss: 2.932076\n","Train Epoch: 11 [2400/3396 (71%)]\tLoss: 2.288378\n","Train Epoch: 11 [2800/3396 (82%)]\tLoss: 3.267369\n","Train Epoch: 11 [3200/3396 (94%)]\tLoss: 3.618626\n","Train Epoch: 12 [0/3396 (0%)]\tLoss: 2.679565\n","Train Epoch: 12 [400/3396 (12%)]\tLoss: 3.887320\n","Train Epoch: 12 [800/3396 (24%)]\tLoss: 2.283671\n","Train Epoch: 12 [1200/3396 (35%)]\tLoss: 2.222838\n","Train Epoch: 12 [1600/3396 (47%)]\tLoss: 2.908564\n","Train Epoch: 12 [2000/3396 (59%)]\tLoss: 2.501314\n","Train Epoch: 12 [2400/3396 (71%)]\tLoss: 1.246778\n","Train Epoch: 12 [2800/3396 (82%)]\tLoss: 2.558931\n","Train Epoch: 12 [3200/3396 (94%)]\tLoss: 3.030718\n","Train Epoch: 13 [0/3396 (0%)]\tLoss: 2.571677\n","Train Epoch: 13 [400/3396 (12%)]\tLoss: 3.283161\n","Train Epoch: 13 [800/3396 (24%)]\tLoss: 2.564845\n","Train Epoch: 13 [1200/3396 (35%)]\tLoss: 2.772520\n","Train Epoch: 13 [1600/3396 (47%)]\tLoss: 3.054506\n","Train Epoch: 13 [2000/3396 (59%)]\tLoss: 2.563622\n","Train Epoch: 13 [2400/3396 (71%)]\tLoss: 3.234219\n","Train Epoch: 13 [2800/3396 (82%)]\tLoss: 2.235096\n","Train Epoch: 13 [3200/3396 (94%)]\tLoss: 2.683874\n","Train Epoch: 14 [0/3396 (0%)]\tLoss: 2.120917\n","Train Epoch: 14 [400/3396 (12%)]\tLoss: 1.983742\n","Train Epoch: 14 [800/3396 (24%)]\tLoss: 2.213406\n","Train Epoch: 14 [1200/3396 (35%)]\tLoss: 2.255024\n","Train Epoch: 14 [1600/3396 (47%)]\tLoss: 1.532443\n","Train Epoch: 14 [2000/3396 (59%)]\tLoss: 2.176508\n","Train Epoch: 14 [2400/3396 (71%)]\tLoss: 1.791719\n","Train Epoch: 14 [2800/3396 (82%)]\tLoss: 2.747073\n","Train Epoch: 14 [3200/3396 (94%)]\tLoss: 2.346827\n","Train Epoch: 15 [0/3396 (0%)]\tLoss: 2.569027\n","Train Epoch: 15 [400/3396 (12%)]\tLoss: 3.597138\n","Train Epoch: 15 [800/3396 (24%)]\tLoss: 2.000087\n","Train Epoch: 15 [1200/3396 (35%)]\tLoss: 2.810335\n","Train Epoch: 15 [1600/3396 (47%)]\tLoss: 2.045371\n","Train Epoch: 15 [2000/3396 (59%)]\tLoss: 2.722593\n","Train Epoch: 15 [2400/3396 (71%)]\tLoss: 2.769840\n","Train Epoch: 15 [2800/3396 (82%)]\tLoss: 2.318068\n","Train Epoch: 15 [3200/3396 (94%)]\tLoss: 2.544645\n","Train Epoch: 16 [0/3396 (0%)]\tLoss: 2.386108\n","Train Epoch: 16 [400/3396 (12%)]\tLoss: 1.522899\n","Train Epoch: 16 [800/3396 (24%)]\tLoss: 2.640945\n","Train Epoch: 16 [1200/3396 (35%)]\tLoss: 2.123162\n","Train Epoch: 16 [1600/3396 (47%)]\tLoss: 2.452120\n","Train Epoch: 16 [2000/3396 (59%)]\tLoss: 1.902268\n","Train Epoch: 16 [2400/3396 (71%)]\tLoss: 2.344145\n","Train Epoch: 16 [2800/3396 (82%)]\tLoss: 3.249181\n","Train Epoch: 16 [3200/3396 (94%)]\tLoss: 3.177644\n","Train Epoch: 17 [0/3396 (0%)]\tLoss: 2.585837\n","Train Epoch: 17 [400/3396 (12%)]\tLoss: 3.275799\n","Train Epoch: 17 [800/3396 (24%)]\tLoss: 3.424208\n","Train Epoch: 17 [1200/3396 (35%)]\tLoss: 1.507773\n","Train Epoch: 17 [1600/3396 (47%)]\tLoss: 13.509832\n","Train Epoch: 17 [2000/3396 (59%)]\tLoss: 2.610710\n","Train Epoch: 17 [2400/3396 (71%)]\tLoss: 2.025090\n","Train Epoch: 17 [2800/3396 (82%)]\tLoss: 1.565472\n","Train Epoch: 17 [3200/3396 (94%)]\tLoss: 3.372523\n","Train Epoch: 18 [0/3396 (0%)]\tLoss: 2.088668\n","Train Epoch: 18 [400/3396 (12%)]\tLoss: 2.731171\n","Train Epoch: 18 [800/3396 (24%)]\tLoss: 2.538252\n","Train Epoch: 18 [1200/3396 (35%)]\tLoss: 1.954039\n","Train Epoch: 18 [1600/3396 (47%)]\tLoss: 1.482936\n","Train Epoch: 18 [2000/3396 (59%)]\tLoss: 1.866403\n","Train Epoch: 18 [2400/3396 (71%)]\tLoss: 3.773607\n","Train Epoch: 18 [2800/3396 (82%)]\tLoss: 2.849210\n","Train Epoch: 18 [3200/3396 (94%)]\tLoss: 2.330088\n","Train Epoch: 19 [0/3396 (0%)]\tLoss: 1.799347\n","Train Epoch: 19 [400/3396 (12%)]\tLoss: 1.999799\n","Train Epoch: 19 [800/3396 (24%)]\tLoss: 2.241008\n","Train Epoch: 19 [1200/3396 (35%)]\tLoss: 5.351095\n","Train Epoch: 19 [1600/3396 (47%)]\tLoss: 1.821418\n","Train Epoch: 19 [2000/3396 (59%)]\tLoss: 2.461259\n","Train Epoch: 19 [2400/3396 (71%)]\tLoss: 2.061701\n","Train Epoch: 19 [2800/3396 (82%)]\tLoss: 2.546766\n","Train Epoch: 19 [3200/3396 (94%)]\tLoss: 2.937767\n"]}]},{"cell_type":"code","source":["torch.save(model_res,'/content/drive/MyDrive/ML_Capstone/project_v7_res_img0(fast)_0003091_nodecay_20ep.pt')\n","model_res = torch.load('/content/drive/MyDrive/ML_Capstone/project_v7_res_img0(fast)_0003091_nodecay_20ep.pt')\n","model_res.eval()\n","ids,preds = test(model_res)\n","\n","import pickle\n","import pandas as pd\n","\n","outfile = '/content/drive/MyDrive/ML_Capstone/submission_v7_img0(fast)_0003091_nodecay_20ep.csv'\n","\n","output_file = open(outfile, 'w')\n","\n","titles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n","         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\n","\n","df = pd.concat([pd.DataFrame(ids), pd.DataFrame.from_records(preds)], axis = 1, names = titles)\n","df.columns = titles\n","df.to_csv(outfile, index = False)\n","print(\"Written to csv file {}\".format(outfile))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QZwI9vZOFjw1","executionInfo":{"status":"ok","timestamp":1670895933245,"user_tz":300,"elapsed":11724,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}},"outputId":"61f9e880-617c-4a93-d6d7-72b7217cabfb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Written to csv file /content/drive/MyDrive/ML_Capstone/submission_v7_img0(fast)_0003091_nodecay_20ep.csv\n"]}]},{"cell_type":"markdown","source":["Conclusion for SGD_hyperparameter choosing: \n","When lr=3e-3, momentum=0.91, 20 epoch, train_batch_size = 2, no lr decay, we get best score on Kaggle."],"metadata":{"id":"rBU1ysbeOacT"}},{"cell_type":"code","source":["def Adam_hyper_tuning(lr_list):  \n","  for i in lr_list:\n","    model_res = Res(input_channels, output_size) # create Res model\n","    model_res.to(device)\n","    print(\"lr=\",i)\n","    train(epoch=0, model=model_res, optimizer=torch.optim.Adam(model_res.parameters(), lr=i),loader=cross_val_train_loader)\n","    print(\"Validation Performance:\",cross_val(model_res))"],"metadata":{"id":"HiaFUdxHPQPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_list = [1e-5,1e-4,1e-3,1e-2,1e-1]\n","Adam_hyper_tuning(lr_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cXyN8-1jPmR_","executionInfo":{"status":"ok","timestamp":1670897677338,"user_tz":300,"elapsed":230040,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}},"outputId":"f1d2c309-a621-4dfd-8727-8e29025cfdd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["lr= 1e-05\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 65.144081\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 49.260761\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 21.110296\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 18.653252\n","Validation Performance: 13.924497604370117\n","lr= 0.0001\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 64.287918\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 14.503418\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 9.274224\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 12.771055\n","Validation Performance: 8.159941673278809\n","lr= 0.001\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 66.451294\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 14.505063\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 11.473140\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 6.911225\n","Validation Performance: 8.506036758422852\n","lr= 0.01\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 66.403252\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 20.115875\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 29.106987\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 22.614956\n","Validation Performance: 26.6864013671875\n","lr= 0.1\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 68.406868\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 19.987238\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 17.728788\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 20.677059\n","Validation Performance: 22.735260009765625\n"]}]},{"cell_type":"markdown","source":["Top 2 Performance\n","1.  lr = 1e-4,  loss on validation set = 8.15\n","2.  lr = 1e-3,  loss on validation set = 8.5"],"metadata":{"id":"fgz4cPWPSXXs"}},{"cell_type":"code","source":["lr_list = [1e-4,2e-4,3e-4,4e-4,5e-4]\n","Adam_hyper_tuning(lr_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Qkj-AcNRzmu","executionInfo":{"status":"ok","timestamp":1670898203646,"user_tz":300,"elapsed":232218,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}},"outputId":"d3ad77db-4834-4993-b71b-f394f4780445"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["lr= 0.0001\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 66.667213\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 13.924146\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 8.433039\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 8.123702\n","Validation Performance: 6.761453628540039\n","lr= 0.0002\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 63.493702\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 12.982480\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 7.902048\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 9.107423\n","Validation Performance: 7.139166355133057\n","lr= 0.0003\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 68.070793\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 10.867259\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 11.598380\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 8.672360\n","Validation Performance: 6.6652913093566895\n","lr= 0.0004\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 66.768654\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 11.016487\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 9.423256\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 6.551960\n","Validation Performance: 7.405411243438721\n","lr= 0.0005\n","Train Epoch: 0 [0/3056 (0%)]\tLoss: 66.077522\n","Train Epoch: 0 [800/3056 (26%)]\tLoss: 7.475839\n","Train Epoch: 0 [1600/3056 (52%)]\tLoss: 10.879752\n","Train Epoch: 0 [2400/3056 (79%)]\tLoss: 7.048964\n","Validation Performance: 7.915044784545898\n"]}]},{"cell_type":"markdown","source":["Top 1 Performance\n","1.  lr = 3e-4,  loss on validation set = 6.66"],"metadata":{"id":"8UVRV26ESmTs"}},{"cell_type":"code","source":["model_res = Res(input_channels, output_size) # create Res model\n","model_res.to(device)\n","optimizer = torch.optim.Adam(model_res.parameters(), lr=3e-4) \n","\n","print(\"lr=\",3e-4)\n","for epoch in range(20):\n","  train(epoch, model=model_res, optimizer=torch.optim.Adam(model_res.parameters(), lr=3e-4),loader=train_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnOlNzMJS_xb","executionInfo":{"status":"ok","timestamp":1670899944658,"user_tz":300,"elapsed":1638998,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}},"outputId":"7a3bda82-326e-4648-c702-b3e40eff8d33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["lr= 0.0003\n","Train Epoch: 0 [0/3396 (0%)]\tLoss: 68.487396\n","Train Epoch: 0 [400/3396 (12%)]\tLoss: 18.962946\n","Train Epoch: 0 [800/3396 (24%)]\tLoss: 16.527212\n","Train Epoch: 0 [1200/3396 (35%)]\tLoss: 13.674988\n","Train Epoch: 0 [1600/3396 (47%)]\tLoss: 10.032792\n","Train Epoch: 0 [2000/3396 (59%)]\tLoss: 9.797058\n","Train Epoch: 0 [2400/3396 (71%)]\tLoss: 5.768251\n","Train Epoch: 0 [2800/3396 (82%)]\tLoss: 6.646792\n","Train Epoch: 0 [3200/3396 (94%)]\tLoss: 4.450601\n","Train Epoch: 1 [0/3396 (0%)]\tLoss: 7.412120\n","Train Epoch: 1 [400/3396 (12%)]\tLoss: 6.709545\n","Train Epoch: 1 [800/3396 (24%)]\tLoss: 6.022833\n","Train Epoch: 1 [1200/3396 (35%)]\tLoss: 7.240652\n","Train Epoch: 1 [1600/3396 (47%)]\tLoss: 4.797465\n","Train Epoch: 1 [2000/3396 (59%)]\tLoss: 5.765735\n","Train Epoch: 1 [2400/3396 (71%)]\tLoss: 5.188417\n","Train Epoch: 1 [2800/3396 (82%)]\tLoss: 5.439257\n","Train Epoch: 1 [3200/3396 (94%)]\tLoss: 5.257957\n","Train Epoch: 2 [0/3396 (0%)]\tLoss: 7.138648\n","Train Epoch: 2 [400/3396 (12%)]\tLoss: 4.166456\n","Train Epoch: 2 [800/3396 (24%)]\tLoss: 4.782614\n","Train Epoch: 2 [1200/3396 (35%)]\tLoss: 2.796328\n","Train Epoch: 2 [1600/3396 (47%)]\tLoss: 3.887902\n","Train Epoch: 2 [2000/3396 (59%)]\tLoss: 4.487632\n","Train Epoch: 2 [2400/3396 (71%)]\tLoss: 4.070063\n","Train Epoch: 2 [2800/3396 (82%)]\tLoss: 5.181314\n","Train Epoch: 2 [3200/3396 (94%)]\tLoss: 3.550399\n","Train Epoch: 3 [0/3396 (0%)]\tLoss: 6.150599\n","Train Epoch: 3 [400/3396 (12%)]\tLoss: 5.803134\n","Train Epoch: 3 [800/3396 (24%)]\tLoss: 5.928061\n","Train Epoch: 3 [1200/3396 (35%)]\tLoss: 4.384101\n","Train Epoch: 3 [1600/3396 (47%)]\tLoss: 3.655182\n","Train Epoch: 3 [2000/3396 (59%)]\tLoss: 5.467686\n","Train Epoch: 3 [2400/3396 (71%)]\tLoss: 3.761634\n","Train Epoch: 3 [2800/3396 (82%)]\tLoss: 5.549683\n","Train Epoch: 3 [3200/3396 (94%)]\tLoss: 2.954143\n","Train Epoch: 4 [0/3396 (0%)]\tLoss: 3.224632\n","Train Epoch: 4 [400/3396 (12%)]\tLoss: 3.449248\n","Train Epoch: 4 [800/3396 (24%)]\tLoss: 2.275792\n","Train Epoch: 4 [1200/3396 (35%)]\tLoss: 3.669190\n","Train Epoch: 4 [1600/3396 (47%)]\tLoss: 4.378088\n","Train Epoch: 4 [2000/3396 (59%)]\tLoss: 2.515894\n","Train Epoch: 4 [2400/3396 (71%)]\tLoss: 4.369214\n","Train Epoch: 4 [2800/3396 (82%)]\tLoss: 2.317325\n","Train Epoch: 4 [3200/3396 (94%)]\tLoss: 4.538144\n","Train Epoch: 5 [0/3396 (0%)]\tLoss: 2.548920\n","Train Epoch: 5 [400/3396 (12%)]\tLoss: 4.420318\n","Train Epoch: 5 [800/3396 (24%)]\tLoss: 2.628276\n","Train Epoch: 5 [1200/3396 (35%)]\tLoss: 2.797817\n","Train Epoch: 5 [1600/3396 (47%)]\tLoss: 2.873688\n","Train Epoch: 5 [2000/3396 (59%)]\tLoss: 3.696426\n","Train Epoch: 5 [2400/3396 (71%)]\tLoss: 3.353476\n","Train Epoch: 5 [2800/3396 (82%)]\tLoss: 2.594265\n","Train Epoch: 5 [3200/3396 (94%)]\tLoss: 2.827511\n","Train Epoch: 6 [0/3396 (0%)]\tLoss: 16.091471\n","Train Epoch: 6 [400/3396 (12%)]\tLoss: 3.616538\n","Train Epoch: 6 [800/3396 (24%)]\tLoss: 2.655267\n","Train Epoch: 6 [1200/3396 (35%)]\tLoss: 2.674363\n","Train Epoch: 6 [1600/3396 (47%)]\tLoss: 4.453432\n","Train Epoch: 6 [2000/3396 (59%)]\tLoss: 2.323771\n","Train Epoch: 6 [2400/3396 (71%)]\tLoss: 2.773616\n","Train Epoch: 6 [2800/3396 (82%)]\tLoss: 2.501099\n","Train Epoch: 6 [3200/3396 (94%)]\tLoss: 1.676300\n","Train Epoch: 7 [0/3396 (0%)]\tLoss: 3.250751\n","Train Epoch: 7 [400/3396 (12%)]\tLoss: 2.962140\n","Train Epoch: 7 [800/3396 (24%)]\tLoss: 1.851980\n","Train Epoch: 7 [1200/3396 (35%)]\tLoss: 3.063130\n","Train Epoch: 7 [1600/3396 (47%)]\tLoss: 2.801236\n","Train Epoch: 7 [2000/3396 (59%)]\tLoss: 2.468862\n","Train Epoch: 7 [2400/3396 (71%)]\tLoss: 4.565983\n","Train Epoch: 7 [2800/3396 (82%)]\tLoss: 3.374337\n","Train Epoch: 7 [3200/3396 (94%)]\tLoss: 2.464983\n","Train Epoch: 8 [0/3396 (0%)]\tLoss: 3.437739\n","Train Epoch: 8 [400/3396 (12%)]\tLoss: 2.703542\n","Train Epoch: 8 [800/3396 (24%)]\tLoss: 2.782558\n","Train Epoch: 8 [1200/3396 (35%)]\tLoss: 3.579098\n","Train Epoch: 8 [1600/3396 (47%)]\tLoss: 2.398488\n","Train Epoch: 8 [2000/3396 (59%)]\tLoss: 3.013693\n","Train Epoch: 8 [2400/3396 (71%)]\tLoss: 3.040858\n","Train Epoch: 8 [2800/3396 (82%)]\tLoss: 2.166367\n","Train Epoch: 8 [3200/3396 (94%)]\tLoss: 2.558673\n","Train Epoch: 9 [0/3396 (0%)]\tLoss: 2.694343\n","Train Epoch: 9 [400/3396 (12%)]\tLoss: 1.971985\n","Train Epoch: 9 [800/3396 (24%)]\tLoss: 3.788976\n","Train Epoch: 9 [1200/3396 (35%)]\tLoss: 2.095724\n","Train Epoch: 9 [1600/3396 (47%)]\tLoss: 3.250777\n","Train Epoch: 9 [2000/3396 (59%)]\tLoss: 1.599379\n","Train Epoch: 9 [2400/3396 (71%)]\tLoss: 4.179993\n","Train Epoch: 9 [2800/3396 (82%)]\tLoss: 3.249880\n","Train Epoch: 9 [3200/3396 (94%)]\tLoss: 3.218667\n","Train Epoch: 10 [0/3396 (0%)]\tLoss: 4.061609\n","Train Epoch: 10 [400/3396 (12%)]\tLoss: 2.263742\n","Train Epoch: 10 [800/3396 (24%)]\tLoss: 3.179312\n","Train Epoch: 10 [1200/3396 (35%)]\tLoss: 2.974923\n","Train Epoch: 10 [1600/3396 (47%)]\tLoss: 1.859275\n","Train Epoch: 10 [2000/3396 (59%)]\tLoss: 2.666971\n","Train Epoch: 10 [2400/3396 (71%)]\tLoss: 3.061723\n","Train Epoch: 10 [2800/3396 (82%)]\tLoss: 3.721557\n","Train Epoch: 10 [3200/3396 (94%)]\tLoss: 3.132441\n","Train Epoch: 11 [0/3396 (0%)]\tLoss: 2.934165\n","Train Epoch: 11 [400/3396 (12%)]\tLoss: 2.361887\n","Train Epoch: 11 [800/3396 (24%)]\tLoss: 2.227329\n","Train Epoch: 11 [1200/3396 (35%)]\tLoss: 2.769166\n","Train Epoch: 11 [1600/3396 (47%)]\tLoss: 3.162637\n","Train Epoch: 11 [2000/3396 (59%)]\tLoss: 3.578678\n","Train Epoch: 11 [2400/3396 (71%)]\tLoss: 2.695484\n","Train Epoch: 11 [2800/3396 (82%)]\tLoss: 4.280134\n","Train Epoch: 11 [3200/3396 (94%)]\tLoss: 2.261588\n","Train Epoch: 12 [0/3396 (0%)]\tLoss: 1.678888\n","Train Epoch: 12 [400/3396 (12%)]\tLoss: 2.210763\n","Train Epoch: 12 [800/3396 (24%)]\tLoss: 2.130808\n","Train Epoch: 12 [1200/3396 (35%)]\tLoss: 3.381098\n","Train Epoch: 12 [1600/3396 (47%)]\tLoss: 1.743663\n","Train Epoch: 12 [2000/3396 (59%)]\tLoss: 2.077776\n","Train Epoch: 12 [2400/3396 (71%)]\tLoss: 2.344435\n","Train Epoch: 12 [2800/3396 (82%)]\tLoss: 3.320487\n","Train Epoch: 12 [3200/3396 (94%)]\tLoss: 2.741788\n","Train Epoch: 13 [0/3396 (0%)]\tLoss: 6.254549\n","Train Epoch: 13 [400/3396 (12%)]\tLoss: 2.621906\n","Train Epoch: 13 [800/3396 (24%)]\tLoss: 2.407357\n","Train Epoch: 13 [1200/3396 (35%)]\tLoss: 1.544667\n","Train Epoch: 13 [1600/3396 (47%)]\tLoss: 1.629127\n","Train Epoch: 13 [2000/3396 (59%)]\tLoss: 2.052111\n","Train Epoch: 13 [2400/3396 (71%)]\tLoss: 1.575216\n","Train Epoch: 13 [2800/3396 (82%)]\tLoss: 2.355201\n","Train Epoch: 13 [3200/3396 (94%)]\tLoss: 2.611292\n","Train Epoch: 14 [0/3396 (0%)]\tLoss: 2.094622\n","Train Epoch: 14 [400/3396 (12%)]\tLoss: 2.935120\n","Train Epoch: 14 [800/3396 (24%)]\tLoss: 2.356530\n","Train Epoch: 14 [1200/3396 (35%)]\tLoss: 1.872698\n","Train Epoch: 14 [1600/3396 (47%)]\tLoss: 1.966731\n","Train Epoch: 14 [2000/3396 (59%)]\tLoss: 2.298782\n","Train Epoch: 14 [2400/3396 (71%)]\tLoss: 1.794801\n","Train Epoch: 14 [2800/3396 (82%)]\tLoss: 2.401607\n","Train Epoch: 14 [3200/3396 (94%)]\tLoss: 2.624316\n","Train Epoch: 15 [0/3396 (0%)]\tLoss: 3.301874\n","Train Epoch: 15 [400/3396 (12%)]\tLoss: 1.536262\n","Train Epoch: 15 [800/3396 (24%)]\tLoss: 3.034647\n","Train Epoch: 15 [1200/3396 (35%)]\tLoss: 2.735728\n","Train Epoch: 15 [1600/3396 (47%)]\tLoss: 1.658090\n","Train Epoch: 15 [2000/3396 (59%)]\tLoss: 2.379885\n","Train Epoch: 15 [2400/3396 (71%)]\tLoss: 2.578287\n","Train Epoch: 15 [2800/3396 (82%)]\tLoss: 2.161824\n","Train Epoch: 15 [3200/3396 (94%)]\tLoss: 2.786384\n","Train Epoch: 16 [0/3396 (0%)]\tLoss: 2.257980\n","Train Epoch: 16 [400/3396 (12%)]\tLoss: 2.317327\n","Train Epoch: 16 [800/3396 (24%)]\tLoss: 1.763227\n","Train Epoch: 16 [1200/3396 (35%)]\tLoss: 2.314785\n","Train Epoch: 16 [1600/3396 (47%)]\tLoss: 2.142426\n","Train Epoch: 16 [2000/3396 (59%)]\tLoss: 2.420054\n","Train Epoch: 16 [2400/3396 (71%)]\tLoss: 2.693196\n","Train Epoch: 16 [2800/3396 (82%)]\tLoss: 1.950671\n","Train Epoch: 16 [3200/3396 (94%)]\tLoss: 2.500743\n","Train Epoch: 17 [0/3396 (0%)]\tLoss: 1.444950\n","Train Epoch: 17 [400/3396 (12%)]\tLoss: 1.829072\n","Train Epoch: 17 [800/3396 (24%)]\tLoss: 2.350248\n","Train Epoch: 17 [1200/3396 (35%)]\tLoss: 3.056091\n","Train Epoch: 17 [1600/3396 (47%)]\tLoss: 3.668294\n","Train Epoch: 17 [2000/3396 (59%)]\tLoss: 3.414671\n","Train Epoch: 17 [2400/3396 (71%)]\tLoss: 2.023132\n","Train Epoch: 17 [2800/3396 (82%)]\tLoss: 2.218894\n","Train Epoch: 17 [3200/3396 (94%)]\tLoss: 1.949275\n","Train Epoch: 18 [0/3396 (0%)]\tLoss: 1.722333\n","Train Epoch: 18 [400/3396 (12%)]\tLoss: 1.954411\n","Train Epoch: 18 [800/3396 (24%)]\tLoss: 1.549941\n","Train Epoch: 18 [1200/3396 (35%)]\tLoss: 1.493505\n","Train Epoch: 18 [1600/3396 (47%)]\tLoss: 2.440202\n","Train Epoch: 18 [2000/3396 (59%)]\tLoss: 2.521612\n","Train Epoch: 18 [2400/3396 (71%)]\tLoss: 2.332312\n","Train Epoch: 18 [2800/3396 (82%)]\tLoss: 1.424866\n","Train Epoch: 18 [3200/3396 (94%)]\tLoss: 1.214547\n","Train Epoch: 19 [0/3396 (0%)]\tLoss: 1.932258\n","Train Epoch: 19 [400/3396 (12%)]\tLoss: 2.814435\n","Train Epoch: 19 [800/3396 (24%)]\tLoss: 1.874458\n","Train Epoch: 19 [1200/3396 (35%)]\tLoss: 2.478239\n","Train Epoch: 19 [1600/3396 (47%)]\tLoss: 2.326984\n","Train Epoch: 19 [2000/3396 (59%)]\tLoss: 1.930922\n","Train Epoch: 19 [2400/3396 (71%)]\tLoss: 1.856313\n","Train Epoch: 19 [2800/3396 (82%)]\tLoss: 1.034397\n","Train Epoch: 19 [3200/3396 (94%)]\tLoss: 1.886804\n"]}]},{"cell_type":"code","source":["torch.save(model_res,'/content/drive/MyDrive/ML_Capstone/project_v7_res_img0(fast)_00003_nodecay_20ep_Adam.pt')\n","model_res = torch.load('/content/drive/MyDrive/ML_Capstone/project_v7_res_img0(fast)_00003_nodecay_20ep_Adam.pt')\n","model_res.eval()\n","ids,preds = test(model_res)\n","\n","import pickle\n","import pandas as pd\n","\n","outfile = '/content/drive/MyDrive/ML_Capstone/submission_v7_img0(fast)_00003_nodecay_20ep_Adam.csv'\n","\n","output_file = open(outfile, 'w')\n","\n","titles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n","         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\n","\n","df = pd.concat([pd.DataFrame(ids), pd.DataFrame.from_records(preds)], axis = 1, names = titles)\n","df.columns = titles\n","df.to_csv(outfile, index = False)\n","print(\"Written to csv file {}\".format(outfile))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qxx2AaniaZdp","executionInfo":{"status":"ok","timestamp":1670900261850,"user_tz":300,"elapsed":12334,"user":{"displayName":"Cat Pinyu Chen","userId":"18163985176633181500"}},"outputId":"2d38ab01-a3d6-4038-c2bf-f93206be4dd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Written to csv file /content/drive/MyDrive/ML_Capstone/submission_v7_img0(fast)_00003_nodecay_20ep_Adam.csv\n"]}]},{"cell_type":"markdown","source":["Conclusion for Adam_hyperparameter choosing: Although from results here, ADAM with best parameters perform better than SGD, on Kaggle, SGD still wins over ADAM. Maybe it's because Kaggle only uses 60% of testing data."],"metadata":{"id":"zwB7zUwyQv-2"}}]}